# DeepAgent: A General Reasoning Agent with Scalable Toolsets
**Source:** 2510.21618v1.pdf  
**Ingestion Date:** 2025-11-28

## Executive Summary
DeepAgent is an innovative framework designed to enhance the capabilities of large reasoning models (LRMs) by integrating autonomous tool discovery and usage within a unified reasoning process. This approach addresses the limitations of traditional agent frameworks that rely on predefined workflows, which often restrict the agent's ability to dynamically interact with complex, real-world environments. By introducing an autonomous memory folding mechanism and a novel reinforcement learning strategy called ToolPO, DeepAgent significantly improves the efficiency and effectiveness of tool-augmented reasoning.

The framework's ability to dynamically retrieve and utilize tools as needed allows it to tackle a wide range of tasks, from general tool-use scenarios to complex downstream applications. Extensive experiments demonstrate that DeepAgent consistently outperforms existing methods, particularly in open-set scenarios where the ability to discover and leverage new tools is crucial. This positions DeepAgent as a powerful solution for developing more general and capable reasoning agents.

## Key Concepts & Principles
- **Autonomous Memory Folding:** A mechanism to compress interaction history into structured memories, enhancing reasoning efficiency and preventing error accumulation.
- **ToolPO (Tool Policy Optimization):** An end-to-end reinforcement learning strategy that uses LLM-simulated APIs for stable training and fine-grained advantage attribution for tool invocation.
- **Dynamic Tool Discovery:** The ability to autonomously search for and utilize tools during task execution, rather than relying on predefined toolsets.
- **Agentic Reasoning Process:** A unified approach that integrates thinking, tool discovery, and action execution within a single reasoning framework.

## Detailed Technical Analysis

### Autonomous Memory Folding
DeepAgent employs a brain-inspired memory architecture comprising episodic, working, and tool memories. This structured memory schema allows the agent to maintain a coherent understanding of its task progress and tool interactions, enabling it to "take a breath" and reconsider its strategy when necessary.

### ToolPO Reinforcement Learning
ToolPO enhances the agent's tool-use capabilities by simulating real-world APIs with LLMs, providing a stable training environment. The strategy includes global and tool-call advantage attribution, which assigns credit to specific tokens responsible for correct tool invocations, ensuring precise learning signals.

### Dynamic Tool Discovery and Usage
DeepAgent's ability to dynamically discover and call tools as needed is a key differentiator from traditional methods. This flexibility allows the agent to adapt to new tasks and environments, making it highly effective in open-set scenarios where predefined tools are insufficient.

## Enterprise Q&A Bank

1. **How does DeepAgent improve upon traditional agent frameworks?**
   - DeepAgent integrates tool discovery and usage within a unified reasoning process, allowing for dynamic interaction with complex environments.

2. **What is the role of autonomous memory folding in DeepAgent?**
   - It compresses past interactions into structured memories, reducing error accumulation and enhancing reasoning efficiency.

3. **How does ToolPO contribute to DeepAgent's performance?**
   - ToolPO provides stable training through LLM-simulated APIs and precise learning signals via fine-grained advantage attribution.

4. **Why is dynamic tool discovery important for real-world applications?**
   - It enables the agent to adapt to new tasks and environments by autonomously finding and using the necessary tools.

5. **What types of tasks can DeepAgent handle effectively?**
   - DeepAgent excels in both general tool-use scenarios and complex downstream applications requiring domain-specific toolsets.

## Actionable Takeaways
- Implement autonomous memory folding to enhance reasoning efficiency in long-horizon tasks.
- Utilize LLM-simulated APIs for stable and cost-effective reinforcement learning training.
- Develop agents capable of dynamic tool discovery to improve adaptability in open-set scenarios.
- Integrate tool usage into a continuous reasoning process for more coherent and effective task execution.

---
**Raw Content:**  
DeepAgent: A General Reasoning Agent with Scalable Toolsets Xiaoxi Li 1 , 2 âˆ— , Wenxiang Jiao 2 , Jiarui Jin 2 , Guanting Dong 1 , Jiajie Jin 1 , Yinuo Wang 2 , Hao Wang 2 , Yutao Zhu 1 , Ji-Rong Wen 1 , Yuan Lu 2 , Zhicheng Dou 1 1 Renmin University of China 2 Xiaohongshu Inc. {xiaoxi_li, dou}@ruc.edu.cn, luyuan3@xiaohongshu.com Abstract Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long- horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent , an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an au- tonomous memory folding mechanism that compresses past in- teractions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advan- tage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, Tool- Hop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms base- lines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real - world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent . Keywords Large Reasoning Models, Autonomous Agents, Tool Retrieval, Mem- ory Mechanism, Reinforcement Learning 1 Introduction The rapid advancement of large language models (LLMs) has in- spired the development of LLM-powered agents, which have found broad applications in scenarios such as web information seeking, software engineering, and personal assistance [ 19 , 39 , 53 ]. Existing agent frameworks predominantly rely on predefined workflows, exemplified by methods like ReAct [ 67 ] and Plan-and-Solve [ 54 ], âˆ— Work done during internship at Xiaohongshu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conferenceâ€™17, Washington, DC, USA Â© 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/xxxxxxx.xxxxxxx 0 10 20 30 40 50 60 70 80 ToolHop Spotify TMDB API-Bank ToolBench (a) General Tool Usage Tasks 25.4 18.4 28.5 39.2 48.0 24.5 20.7 30.0 40.7 54.5 32.1 32.5 56.0 48.6 46.5 46.0 63.1 72.0 49.6 66.5 DeepAgent-32B CodeAct-32B ReAct-GPT-4o ReAct-32B 0% 25% 50% 75% 100% ALFWorld (Success) HLE (Text) HLE (MM) GAIA (File) GAIA (MM) GAIA (Text) WebShop (Score) WebShop (Success) ALFWorld (Path) (b) Downstream Applications DeepAgent-32B WebThinker-32B ReAct-GPT-4o ReAct-32B Figure 1: Overall performance on (a) general tool usage tasks and (b) downstream applications (best score as 100%). which employ structured planning processes and iterative â€œReason- Act-Observeâ€ cycles as illustrated in Figure 2(a). Although effective in simpler tasks, these approaches suffer from several critical limita- tions: (1) lack of autonomy in execution steps and overall procedure; (2) inability to dynamically discover tools during task execution; (3) deficiency in fully autonomous management of interactive mem- ory; and (4) insufficient depth and coherence in reasoning about the entire task. These fundamental shortcomings severely constrain the agentsâ€™ ability to tackle real-world problems, particularly for complex tasks that demand general tool-use and long-horizon in- teraction with the environment. Recently, the advent of large reasoning models (LRMs) has demon- strated the capability to solve complex problems in domains like mathematics, programming, and scientific reasoning through a step- by-step â€œslow thinkingâ€ process [ 2 , 28 ]. However, many real-world tasks necessitate the use of external tools for their completion. While some studies have explored new paradigms for integrat- ing tool use within the reasoning process, such as Search-o1 [ 25 ], DeepResearcher [ 74 ], and ToRL [ 27 ], these approaches are often restricted to a limited set of predefined tools, such as web search, page browsing, and coding (Figure 2(b)). This constrained set of tools significantly hinders their applicability to a wide range of complex, real-world scenarios. To address these challenges, we introduce DeepAgent , an end- to-end deep reasoning agent that can complete an entire task by dynamically retrieving and calling tools within a single, coherent agentic reasoning process. As depicted in Figure 2(c), DeepAgent operates by autonomously thinking, searching for tools, and execut- ing actions. This paradigm shifts away from traditional, predefined workflows that rely on predefined tools, task planning, and iterative tool use, where each generation step focuses only on the immediate objective. Instead, DeepAgent maintains a global perspective on arXiv:2510.21618v1 [cs.AI] 24 Oct 2025

Conferenceâ€™17, July 2017, Washington, DC, USA Xiaoxi Li et al. Tool Search o1/o3 QwQ R1 Reasoning LLMs o1/o3 QwQ R1 Reasoning LLMs (a) Traditional Agent Workflows (b) Deep Research Agents (c) Ours: DeepAgent Userâ€™s Task Research Tools Page Browsing Web Search Code Execution Userâ€™s Task Scalable Toolsets Spotify TMDB Rapid APIs What if we scale it to any real - world toolsets? Okay, so I have the question about ... First, I need to know ... Let me search for that: <search>...</search> <result>...</result> ... Then, to calculate ... <python>...</python> <result>...</result> ... Now, I can conclude the final answer ... Think w/ Limited Tools Okay, so I have the task about ... First, I need to find available tools for ...: <tool_search>... </tool_search> <result>...</result> ... Hmm, there's a tool named ..., let me try it: <tool_call>...</tool_call> <result>...</result> ... Okay, it works. Next, ... Now, I have ... The task is then completed. Think w/ Scalable Toolsets Robot MCPs Userâ€™s Task Pre - defined cycles, lack of antonomy. Step 1: Plan next steps: ... Step 2: Act: Call tool ... Observe: Result ... Step 3: Think: I have ... , ... Step 4: Act: Call tool ... Observe: Result ... ... Step n: Based on above info, the final answer is ... Iterative LLM Generation 4o Qwen Llama Traditional LLMs Avaiable Tools Figure 2: Comparison of agent paradigms: (a) Traditional agents with predefined workflows, (b) Deep Research agents that can autonomously call limited tools, and (c) Our DeepAgent, a fully autonomous reasoning agent that dynamically discovers and invokes helpful tools, all within a continuous agentic reasoning process. the entire task, unconstrained by the need to deliberate on spe- cific, isolated operations. Tools are not pre-retrieved in advance but are dynamically discovered on an as-needed basis, thereby fully unlocking the autonomous potential of the large reasoning model. To empower DeepAgent to thoroughly and robustly explore new tools and navigate complex environments during long-horizon in- teractions, we equip it with memory management capabilities. We introduce an Autonomous Memory Folding strategy that allows DeepAgent to consolidate its previous thoughts and interaction history into a structured memory schema at any point during its thinking before resuming the agentic reasoning process. This mech- anism not only saves tokens and enhances reasoning efficiency over extended interactions but also provides the agent an opportunity to â€œtake a breathâ€, preventing it from becoming trapped in wrong exploration paths and enabling it to reconsider its strategy, thus improving the overall success rate. To mitigate information loss during this folding process, we introduce a brain-inspired memory architecture comprising episodic memory, working memory, and tool memory, all structured with an agent-usable data schema to ensure the stability and utility of the compressed memory. To enhance DeepAgentâ€™s proficiency in mastering these mecha- nisms, we propose ToolPO , an end-to-end reinforcement learning (RL) training method tailored for general tool use. Existing agentic RL training in general domains presents two significant challenges: (1) The reliance on a multitude of real-world APIs during training can lead to instability, slow execution, and high costs. To prevent this, we leverage LLM-simulated APIs , which enhance the stability and efficiency of the training process. (2) A sparse reward based solely on the final outcome is often insufficient to guarantee the accuracy of intermediate tool calls. We address this by implement- ing tool-call advantage attribution , which precisely assigns credit to the specific tokens responsible for correct tool invocations, thereby providing a more granular and effective learning signal. We conduct extensive experiments on a wide range of bench- marks. For (1) General Tool-Use Tasks , we evaluate DeepAgent on ToolBench, API-Bank, TMDB, Spotify, and ToolHop, which fea- ture toolsets scaling from tens to over ten thousand distinct tools. For (2) Downstream Applications , we test its performance on ALFWorld, WebShop, GAIA, and Humanityâ€™s Last Exam (HLE), which require the use of domain-specific toolsets. The overall re- sults in Figure 1 show that DeepAgent achieves superior perfor- mance across all scenarios. Our main contributions are summarized as follows: (1) We propose DeepAgent, the first agentic framework that enables reasoning models to autonomously think, discover tools, and execute actions within a unified reasoning process, empowering LRMs to harness toolsets of arbitrary scale and generalize to complex real-world tasks. (2) We introduce an autonomous memory folding mechanism, com- plemented by a brain-inspired memory design. This endows the agent with the ability to â€œtake a breathâ€ and reconsider its explo- ration strategies following unsuccessful attempts. (3) We propose an end-to-end reinforcement learning training method- ology for general-purpose tool use, ensuring stability and effi- ciency in large-scale tool execution during training, as well as accuracy in tool invocation during reasoning. (4) We conduct extensive experiments across eight benchmarks, demonstrating DeepAgentâ€™s superior tool-use capabilities and high adaptability to real-world tasks. 2 Related Work 2.1 Large Reasoning Models Large Reasoning Models (LRMs) [ 5 , 16 ] have demonstrated signifi- cant performance improvements in mathematical, scientific, and coding tasks by employing step-by-step slow thinking processes before generating final responses. Existing research has explored various approaches to elicit extended Chain-of-Thought (CoT) rea- soning [ 58 ] from models, including data synthesis for Supervised Fine-Tuning (SFT) [ 33 , 36 , 69 ], and end-to-end RL [ 5 , 14 ]. Addition- ally, substantial work has investigated optimization strategies for reasoning models, such as advanced RL training algorithms [ 70 , 73 ] and improving reasoning efficiency [ 3 , 65 ]. However, models re- lying solely on parametric knowledge face inherent limitations and cannot interact with the real world. Recent studies have be- gun exploring tool-augmented reasoning approaches, including Search-o1 [ 25 ], Search-R1 [ 18 ], ToRL [ 27 ], DeepResearcher [ 74 ], and SimpleTIR [ 63 ]. However, these methods typically support only

DeepAgent: A General Reasoning Agent with Scalable Toolsets Conferenceâ€™17, July 2017, Washington, DC, USA D EEP A GENT : A General Reasoning Agent with Scalable Toolsets Get Relevant Tools with Tool Retriever Call Tools and Get Env. Feedback Memory Folding Module S Main Reasoning Process I r C I r E I C S r reasoning step search tools returned info. call tools End of Task a reasoning step multiple steps End-to-end RL with ToolPO Tool Retriever Tool Index Tool Executor Environment F First, I should search for ... tools. Ah, there's a tool ..., let me try it. r Now Iâ€™ve achieved the userâ€™s goal. Interaction History Memory Folding Module Folded Memory Start a New Round F fold prev. memory r What if the task doesn't go smoothly? I'm stuck here. Maybe I should start a new round. a new situation Userâ€™s Tasks Scalable Toolsets General/custom. tasks Movie Music Robotics Shopping RapidAPI (16k tools) ToolHop (3.9k tools) Custom. (1~1k tools) Userâ€™s Task Interaction History S I r C I r r F Folded Memory Episode Memory: long-term task progress. Working Memory: short-term task status. Tool Memory: tool using experence. Auxiliary LLM Parallel Mem. Generation LLM-friendly Data Schema Userâ€™s Task Policy Model ( D EEP A GENT ) Tool Simulator (Auxiliary LLM) Tool Server Web Search Page Browser Code Executor Visual QA Tool Retriever Robotic Env. Shopping Env. Rapid APIs Costly Unstable Rollout Trajectories Rollout C I r r r Task-level Reward Full Seq. Advantage Update Tool Call. Advantage Tool Call. Reward E Figure 3: Overview of the DeepAgent framework. The main reasoning model autonomously discovers tools, executes actions, and folds previous memory to restart with structured memories, all within a unified thinking process. The DeepAgent is trained end-to-end with ToolPO, an RL method that uses a tool simulator to simulate large-scale real-world tool APIs, and rewards both final task success and correct intermediate tool calls through fine-grained advantage attribution. a limited set of research-oriented tools, such as web search, page browsing, and code execution, which constrains their applicability to real-world scenarios that demand access to more diverse tools. 2.2 Autonomous Agents LLM-powered autonomous agents accomplish real-world tasks by invoking external tools to interact with their environment [ 7 , 15 , 21 , 23 , 30 , 38 , 41 , 52 , 53 , 59 , 72 ]. Current agent methodologies, including ReAct [ 67 ], Plan-and-Solve [ 54 ], Reflextion [ 45 ], and CodeAct [ 56 ], predominantly follow predefined workflows with fixed execution patterns. This rigid structure limits their ability to fully leverage the autonomous decision-making and deep reasoning capabilities of advanced reasoning models. Recent efforts have investigated training LLMs to autonomously invoke tools through data synthesis and SFT methods [ 9 , 48 , 62 ] and RL training frameworks [ 4 , 6 , 8 , 10 , 11 , 17 , 22 , 29 , 31 , 49 , 57 , 60 ]. However, most existing methods rely on pre-selected, labeled tools, which limit their applicability to real-world scenarios. Real-world tasks are highly variable and require access to diverse toolsets that cannot be predetermined, aligning with the emerging Model Context Protocol (MCP) [ 13 ] paradigm. Although some prior work has explored tool retrieval mechanisms [ 37 , 43 , 55 ], most approaches conduct only a single upfront retrieval step and incorporate the retrieved tools, with limited exploration of dynamic tool discovery during task execution. Therefore, we aim to develop a deep reasoning agent capable of dynamically discovering and invoking helpful tools from scalable toolsets to address more generalized real-world tasks. 3 Methodology In this section, we first formulate the task of autonomous agentic reasoning. Then, we provide a detailed overview of the DeepA- gent framework. Finally, we elaborate on the core components of DeepAgent, including the mechanism for autonomous tool use and memory folding, the brain-inspired memory schema, and our end-to-end reinforcement learning training method, ToolPO. 3.1 Problem Formulation We frame the agentâ€™s task as a sequential decision-making process. The agent receives a user-provided question ğ‘„ and an instruction ğ¼ , and interacts with an environment over a series of steps ğ‘¡ = 1 , . . . ,ğ‘‡ to accomplish the specified goal. The environment provides access to a collection of tools T at an arbitrary scale. At each step ğ‘¡ , the agentâ€™s state ğ‘  ğ‘¡ consists of the history of all previous actions and their resulting observations, i.e., ğ‘  ğ‘¡ = ( ğ‘ 1 , ğ‘œ 1 , . . . , ğ‘ ğ‘¡ âˆ’ 1 , ğ‘œ ğ‘¡ âˆ’ 1 ) . The agent, driven by a policy ğœ‹ parameter- ized by ğœƒ , selects an action ğ‘ ğ‘¡ based on the current state, the user question, and the instruction: ğ‘ ğ‘¡ âˆ¼ ğœ‹ ğœƒ (Â·| ğ‘  ğ‘¡ , ğ‘„, ğ¼ ) . (1) An action ğ‘ ğ‘¡ can be one of four types: â€¢ Internal Thought ( ğ‘ think ğ‘¡ ) : A textual reasoning step generated by the LRM to analyze the problem or plan its next steps. The corresponding observation ğ‘œ ğ‘¡ is typically empty. â€¢ Tool Search ( ğ‘ search ğ‘¡ ) : A natural language query ğ‘ ğ‘  to find rel- evant tools from the toolset T . The observation ğ‘œ ğ‘¡ is a list of retrieved tools. â€¢ Tool Call ( ğ‘ call ğ‘¡ ) : The invocation of a specific tool ğœ âˆˆ T with a set of arguments. The observation ğ‘œ ğ‘¡ is the execution result returned by the tool. â€¢ Memory Fold ( ğ‘ fold ğ‘¡ ) : A special action to compress the interac- tion history ğ‘  ğ‘¡ into a structured memory summary. The subse- quent state ğ‘  ğ‘¡ + 1 is then initialized with this compressed memory. The sequence of states, actions, and observations forms a trajec- tory ğœ = ( ğ‘  1 , ğ‘ 1 , ğ‘œ 1 , . . . , ğ‘  ğ‘‡ , ğ‘ ğ‘‡ , ğ‘œ ğ‘‡ ) . The process terminates when the

Conferenceâ€™17, July 2017, Washington, DC, USA Xiaoxi Li et al. agent completes the task or reaches a maximum step limit. The ob- jective is to learn an optimal policy ğœ‹ âˆ— ğœƒ that maximizes the expected cumulative reward for a given task: ğœ‹ âˆ— ğœƒ = arg max ğœ‹ ğœƒ E ğœ âˆ¼ ğœ‹ ğœƒ [ ğ‘… ( ğœ )] , (2) where ğ‘… ( ğœ ) is a reward function that evaluates the overall success of the trajectory ğœ . 3.2 Overview of the DeepAgent Framework As illustrated in Figure 3, the DeepAgent framework is architected around a main reasoning process, which is supported by several auxiliary mechanisms to ensure robustness and efficiency. â€¢ Main Reasoning Process : The core of DeepAgent is a powerful large reasoning model that drives the entire task-completion process. In a single stream of thought, the LRM autonomously reasons about the task, dynamically discovers necessary tools, executes actions, and manages its own memory. This unified ap- proach departs from traditional, rigid agent workflows, allowing the LRM to maintain a global perspective on the task. â€¢ Auxiliary Mechanisms : DeepAgent employs an auxiliary LLM to handle complex interactions with large toolsets and manage long histories. This background model enhances system stability by: (1) filtering and summarizing retrieved tool documentation if itâ€™s too lengthy, (2) denoising and condensing verbose informa- tion returned from tool calls, and (3) compressing long interaction histories into a structured memory. This division of labor allows the main LRM to concentrate on high-level strategic reasoning. 3.3 Autonomous Tool Search and Calling DeepAgentâ€™s main LRM performs all actions by generating specific textual prompts within its continuous reasoning process. These actions are then intercepted and executed by the system. Tool Search. When the agent determines it needs a tool, it gen- erates a tool search query ğ‘ ğ‘  encapsulated within special tokens: <tool_search> ğ‘ ğ‘  </tool_search> . The systemâ€™s tool retriever operates via dense retrieval. First, we build an index by pre-computing an embedding ğ¸ ( ğ‘‘ ğ‘– ) for the documentation ğ‘‘ ğ‘– of each tool ğœ ğ‘– âˆˆ T using an embedding model ğ¸ . During inference, given the query ğ‘ ğ‘  , the system retrieves the top- ğ‘˜ tools by ranking them based on the cosine similarity sim (Â· , Â·) : T retrieved = top-k ğœ ğ‘– âˆˆ T ( sim ( ğ¸ ( ğ‘ ğ‘  ) , ğ¸ ( ğ‘‘ ğ‘– ))) . (3) The retrieved tool documentation is then processed by the auxiliary LLM â€”summarized if too lengthy, otherwise provided directlyâ€” and returned to the main LRMâ€™s context: <tool_search_result> relevant tools </tool_search_result> . Tool Call. To execute a tool, the agent generates a structured call including the toolâ€™s name and arguments: <tool_call> {"name": "tool_name", "arguments": ...} </tool_call> . The framework parses this call, executes the tool, and captures the output. This output is, if necessary, summarized by the auxiliary LLM to ensure it is con- cise and helpful, before being fed back into the reasoning context: <tool_call_result> helpful information </tool_call_result> . 3.4 Autonomous Memory Folding and Brain-Inspired Memory Schema The agent can trigger memory folding at any logical point in its reasoning processâ€”such as after completing a sub-task or realizing an exploration path was incorrectâ€”by generating a special token: <fold_thought> . Upon detecting this token, the system initiates the memory folding process. The auxiliary LLM (parameterized by ğœƒ aux ) processes the entire preceding interaction history ğ‘  ğ‘¡ and generates three structured memory components in parallel: ( ğ‘€ ğ¸ , ğ‘€ ğ‘Š , ğ‘€ ğ‘‡ ) = ğ‘“ compress ( ğ‘  ğ‘¡ ; ğœƒ aux ) . (4) These compressed episodic ( ğ‘€ ğ¸ ), working ( ğ‘€ ğ‘Š ), and tool ( ğ‘€ ğ‘‡ ) memories then replace the raw interaction history, enabling the agent to proceed with a refreshed and condensed view of its progress while avoiding entrapment in incorrect exploration paths. Inspired by human cognitive systems, the structured memory ğ‘€ ğ‘¡ is composed of three distinct components that are generated in parallel: ğ‘€ ğ‘¡ = ( ğ‘€ ğ¸ , ğ‘€ ğ‘Š , ğ‘€ ğ‘‡ ) , where ğ‘€ ğ¸ , ğ‘€ ğ‘Š , ğ‘€ ğ‘‡ denote episodic, working, and tool memories, respectively. â€¢ Episodic Memory ( ğ‘€ ğ¸ ) : This component serves as a high-level log of the task, recording key events, major decision points, and sub-task completions. It provides the agent with long-term con- text regarding the overall task structure and its overarching goals. â€¢ Working Memory ( ğ‘€ ğ‘Š ) : This contains the most recent infor- mation, such as the current sub-goal, obstacles encountered, and near-term plans. It is the core component that ensures the conti- nuity of the agentâ€™s reasoning across the memory fold. â€¢ Tool Memory ( ğ‘€ ğ‘‡ ) : This consolidates all tool-related interac- tions, including which tools have been used, how they were invoked, and their effectiveness. It allows the agent to learn from its experiences, refining its tool selection and usage strategies. To ensure that the compressed memory is stable and easily parsed by the agent, we employ an agent-usable data schema in JSON format instead of unstructured natural language. This structured format offers two main benefits: it maintains a controllable and predictable structure, and it mitigates the loss of critical details that can occur when summarizing long-form text. Details of the data schema are provided in Appendix D. 3.5 End-to-end RL Training with ToolPO We train DeepAgent end-to-end with Tool Policy Optimization (ToolPO), an RL approach designed for general tool-using agents. Training Data Collection. We first collect a diverse training dataset spanning four categories. To instill general tool-use capabilities, we use ToolBench [ 37 ]. For real-world interaction , we leverage ALFWorld [ 46 ] and WebShop [ 66 ]. To enhance deep research skills, we incorporate data from WebDancer [ 59 ] and WebShaperQA [ 50 ]. Lastly, to improve mathematical reasoning with code, we use DeepMath [12]. Further details are available in Appendix A.1. Tool Simulator. Training an agent that interacts with thousands of real-world APIs is often impractical due to instability, latency, and cost. To address this, we develop an LLM-based Tool Simulator . This simulator, powered by an auxiliary LLM, mimics the responses of real-world APIs (e.g., RapidAPI). This approach provides a stable, efficient, and low-cost environment for robust RL training.

DeepAgent: A General Reasoning Agent with Scalable Toolsets Conferenceâ€™17, July 2017, Washington, DC, USA Table 1: Main results on general tool usage tasks, encompassing scenarios with both labeled tools and open-set tool retrieval over large-scale toolsets. We report Pass@1 metric for all tasks. For 32B models, the best results are in bold and the second are underlined. Results from larger or closed-sourced models are in gray color for reference. Method Backbone ToolBench API-Bank TMDB Spotify ToolHop Success Path Success Path Success Path Success Path Correct Path Scenario 1: Completing Tasks w/ Ground-truth Tools Workflow-based Methods ReAct Qwen2.5-32B 41.0 64.7 60.4 68.3 46.0 65.3 29.8 56.3 37.6 49.1 CodeAct Qwen2.5-32B 53.0 68.3 62.4 70.6 48.0 67.4 33.3 58.7 34.7 48.8 Plan-and-Solve Qwen2.5-32B 52.0 65.4 58.4 67.5 51.0 71.6 28.1 54.8 39.2 49.7 ReAct QwQ-32B 52.0 61.6 73.3 78.6 43.0 65.3 47.4 69.4 47.4 51.6 CodeAct QwQ-32B 54.0 63.4 74.3 79.4 55.0 74.5 52.6 75.4 43.2 53.4 Plan-and-Solve QwQ-32B 55.0 64.7 70.3 75.4 48.0 61.3 49.1 70.6 45.4 50.6 ReAct Qwen2.5-72B 56.0 69.3 73.3 78.6 47.0 67.7 57.9 76.6 44.8 55.4 ReAct GPT-4o 52.0 53.9 79.2 83.3 77.0 89.3 47.4 70.6 40.0 53.7 ReAct DeepSeek-R1 57.0 68.3 71.3 76.2 76.0 89.0 64.9 81.3 50.2 61.8 Autonomous Tool Usage within Reasoning DeepAgent-32B-Base QwQ-32B 63.0 74.3 76.2 81.0 85.0 92.0 70.2 89.3 49.1 59.8 DeepAgent-32B-RL QwQ-32B 69.0 78.6 75.3 80.2 89.0 94.8 75.4 92.0 51.3 62.5 Scenario 2: Completing Tasks w/ Open-Set Tool Retrieval Workflow-based Methods ReAct Qwen2.5-32B 55.0 20.8 16.0 42.0 11.0 34.5 7.0 25.4 13.2 17.9 CodeAct Qwen2.5-32B 51.0 19.0 22.0 49.6 19.0 46.8 10.5 31.6 12.7 17.4 Plan-and-Solve Qwen2.5-32B 54.0 20.4 18.0 42.8 15.0 40.5 8.8 26.3 12.0 16.3 ReAct QwQ-32B 44.0 19.0 20.0 52.7 18.0 40.3 22.8 45.5 27.1 22.3 CodeAct QwQ-32B 48.0 21.6 16.0 45.0 31.0 52.8 24.6 49.6 29.0 26.1 Plan-and-Solve QwQ-32B 45.0 19.6 18.0 44.3 24.0 46.8 19.3 42.7 25.7 20.8 ReAct Qwen2.5-72B 52.0 21.6 14.0 38.9 28.0 50.7 21.1 48.5 21.1 19.9 ReAct GPT-4o 41.0 28.9 18.0 42.8 35.0 56.8 17.5 26.3 24.1 28.6 ReAct DeepSeek-R1 47.0 22.3 12.0 57.3 34.0 53.1 29.8 51.7 36.2 32.9 Autonomous Tool Retrieval and Usage within Reasoning DeepAgent-32B-Base QwQ-32B 60.0 35.7 22.0 61.8 52.0 71.8 49.1 68.6 38.4 40.3 DeepAgent-32B-RL QwQ-32B 64.0 37.2 24.0 64.9 55.0 74.3 50.9 74.4 40.6 40.5 Global and Tool-Call Advantage Attribution. For each input prompt, we sample a group of ğ¾ trajectories { ğœ 1 , . . . , ğœ ğ¾ } . ToolPO defines two distinct reward components. The first is a reward for overall task success, ğ‘… succ ( ğœ ) , which is a task success score reflecting the quality of the final outcome (e.g., the accuracy of the final answer). The second is a tool-call reward, ğ‘… action ( ğœ ) , which reflects the qual- ity of intermediate actions. This action-level reward is composed of rewards for correct tool invocations and efficient memory fold- ing. Specifically, ğ‘… action ( ğœ ) = ğœ† 1 Ã ğ‘‡ ğ‘¡ = 1 ğ¶ ( ğ‘ call ğ‘¡ ) + ğœ† 2 ğ‘† pref ( ğœ ) , where ğ¶ ( ğ‘ call ğ‘¡ ) is 1 if a tool call is correct and 0 otherwise. ğ‘† pref ( ğœ ) is a preference score encouraging efficient use of memory folding, de- fined by comparing a trajectory with folding ( ğœ fold ) to one without ( ğœ direct ): ğ‘† pref = ( ğ¿ ( ğœ direct ) âˆ’ ğ¿ ( ğœ fold ))/( ğ¿ ( ğœ direct ) + ğ¿ ( ğœ fold )) . Based on these rewards, we compute two separate group-relative advantages. The task success advantage for trajectory ğœ ğ‘˜ is: ğ´ succ ( ğœ ğ‘˜ ) = ğ‘… succ ( ğœ ğ‘˜ ) âˆ’ 1 ğ¾ âˆ‘ï¸ ğ¾ ğ‘— = 1 ğ‘… succ ( ğœ ğ‘— ) . (5) This advantage is attributed to all generated tokens in the trajec- tory, providing a global learning signal. Similarly, the action-level advantage is: ğ´ action ( ğœ ğ‘˜ ) = ğ‘… action ( ğœ ğ‘˜ ) âˆ’ 1 ğ¾ âˆ‘ï¸ ğ¾ ğ‘— = 1 ğ‘… action ( ğœ ğ‘— ) . (6) Crucially, this advantage is attributed only to the specific tokens that constitute the tool call and memory folding actions. This fine- grained credit assignment provides a more targeted signal for learn- ing correct and efficient tool use. Optimization Objective. The total advantage for a given token ğ‘¦ ğ‘– in trajectory ğœ ğ‘˜ is the sum of the global and local advantages: ğ´ ( ğ‘¦ ğ‘– ) = ğ´ succ ( ğœ ğ‘˜ ) + ğ‘€ ( ğ‘¦ ğ‘– ) Â· ğ´ action ( ğœ ğ‘˜ ) , (7) where ğ‘€ ( ğ‘¦ ğ‘– ) is a mask that is 1 if ğ‘¦ ğ‘– is part of a tool-call or memory- fold token sequence, and 0 otherwise. ToolPO then optimizes the policy using a clipped surrogate objective function: L ToolPO ( ğœƒ ) = E ğœ ğ‘˜ hâˆ‘ï¸ | ğœ ğ‘˜ | ğ‘– = 1 min  ğœŒ ğ‘– ( ğœƒ ) ğ´ ( ğ‘¦ ğ‘– ) , clip ( ğœŒ ğ‘– ( ğœƒ ) , 1 âˆ’ ğœ–, 1 + ğœ– ) ğ´ ( ğ‘¦ ğ‘– ) i , (8) Here, ğœŒ ğ‘– ( ğœƒ ) = ğœ‹ ğœƒ ( ğ‘¦ ğ‘– | ğ‘¦ < ğ‘– ,ğ‘  ) ğœ‹ ğœƒ old ( ğ‘¦ ğ‘– | ğ‘¦ < ğ‘– ,ğ‘  ) is the probability ratio for token ğ‘¦ ğ‘– . This objective encourages the model to increase the probability of both intermediate actions and end-to-end task accomplishment that exhibit positive relative advantage, thereby ensuring stable and effective policy updates.

Conferenceâ€™17, July 2017, Washington, DC, USA Xiaoxi Li et al. Table 2: Main results on downstream task applications, spanning Embodied AI (ALFWorld), Online Shopping (WebShop), General AI Assistants (GAIA), and Humanityâ€™s Last Exam (HLE). We report Pass@1 for all tasks. For 32B models, the best results are in bold and the second are underlined . Results from larger or closed-sourced models are in gray color for reference. Method Backbone ALFWorld WebShop GAIA HLE Success Path Success Score Text MM File All Text MM All Completing Tasks w/ Task-specific Toolsets Workflow-based Methods ReAct Qwen2.5-32B 60.4 79.1 6.0 28.8 25.2 16.7 13.2 21.2 6.5 7.1 6.6 CodeAct Qwen2.5-32B 65.7 83.3 12.4 34.5 28.2 20.8 18.4 24.8 7.5 8.0 7.6 Reflextion Qwen2.5-32B 66.4 86.0 9.2 31.6 29.1 20.8 18.4 25.5 5.9 5.3 5.8 Plan-and-Solve Qwen2.5-32B 63.4 80.4 7.6 29.3 27.2 16.7 15.8 23.0 7.2 6.2 7.0 ReAct QwQ-32B 82.1 87.8 17.2 45.3 35.0 8.3 36.8 31.5 13.2 8.8 12.2 CodeAct QwQ-32B 78.4 86.2 18.0 46.4 38.8 20.8 31.6 34.5 14.2 8.0 12.8 Reflextion QwQ-32B 85.1 88.4 21.6 50.4 37.9 20.8 36.8 35.2 11.9 7.1 10.8 Plan-and-Solve QwQ-32B 79.1 84.7 16.0 43.8 36.9 16.7 34.2 33.3 12.9 9.7 12.2 AgentLM* Llama2-70B 86.0 - - 64.9 - - - - - - - ReAct Qwen2.5-72B 86.5 86.5 22.0 44.5 32.0 20.8 31.6 30.3 9.0 8.0 8.8 ReAct DeepSeek-R1 79.1 85.8 19.6 49.7 43.7 29.2 39.5 40.6 14.2 8.8 13.0 ReAct GPT-4o 65.7 87.8 15.6 52.5 35.0 16.7 36.8 32.7 13.2 10.6 12.6 ReAct Claude-4 93.3 91.5 20.4 56.6 56.3 37.5 52.6 52.7 15.5 16.8 15.8 Autonomous Tool Usage within Reasoning Deep Research OpenAI (o3) - - - - - - - 67.4 - - 26.6 WebThinker QwQ-32B - - - - 48.5 25.0 13.2 37.0 14.2 8.8 13.0 HiRA QwQ-32B 84.3 87.6 23.2 51.9 44.7 33.3 42.1 42.5 14.5 10.6 13.6 DeepAgent-32B-Base QwQ-32B 88.1 91.4 32.0 55.4 49.5 37.5 44.7 46.7 19.1 13.3 17.8 DeepAgent-32B-RL QwQ-32B 91.8 92.0 34.4 56.3 58.3 33.3 52.6 53.3 21.7 15.0 20.2 4 Experimental Settings 4.1 Tasks and Datasets We conduct extensive experiments on a wide range of benchmarks, including general tool-use and downstream applications. General Tool-Use. These benchmarks encompass a broad range of distinct tools, scaling from tens to over ten thousand, making them ideal for evaluating the scalability of different approaches. We utilize four representative scenarios: ToolBench [ 37 ] , based on over 16,000 real-world APIs, for which we use the G3 subset requiring multi-step, multi-tool calls; API-Bank [ 24 ] , which in- cludes 314 human-annotated dialogues with 73 APIs and 753 API calls, to assess planning, retrieval, and calling capabilities; Rest- Bench [ 47 ] , comprising scenarios from the TMDB movie database (54 tools, avg. 2.3 calls/question) and the Spotify music player (40 tools, avg. 2.6 calls/question) to simulate typical REST applications; and ToolHop [ 68 ] , a multi-hop reasoning dataset with 3,912 locally executable tools that necessitate 3 to 7 sequential tool calls per task. For these tasks, we adopt two settings: given ground-truth tools and given entire toolsets with tool retrieval capabilities. Downstream Applications. We evaluate our approach on sev- eral downstream applications that require domain-specific toolsets. These include ALFWorld [ 46 ] , a text-based embodied AI task where agents complete goals using nine basic actions (e.g., move, take); WebShop [ 66 ] , an online shopping environment with â€˜searchâ€™ and â€˜clickâ€™ actions to fulfill usersâ€™ specific product purchasing re- quirements; GAIA [ 32 ] , a complex information-seeking benchmark where we equip the agent with tools for web search, page brows- ing, Visual Question Answering (VQA), code compilation, and file reading; and Humanityâ€™s Last Exam (HLE) [ 35 ] , a set of highly difficult reasoning problems, for which we provide code, search, page browsing, and VQA tools. These benchmarks test the agentâ€™s ability to perform long-horizon planning and robust interaction in complex, real-world scenarios. For this category of tasks, we provide agents with task-specific toolsets. 4.2 Baselines Our baselines include: (1) Workflow-based Methods : ReAct [ 67 ] alternates explicit reasoning with environment actions in a Reason- Act-Observe loop. CodeAct [ 56 ] expresses actions as executable Python code that runs in an interpreter. Plan-and-Solve [ 54 ] first sketches a high-level plan and then executes it step by step. Re- flexion [ 44 ] enhances learning through verbal self-reflection after failed attempts. AgentLM [ 71 ] uses instruction tuning to enhance general agent capabilities of LLMs. (2) Autonomous Tool Usage within Reasoning : WebThinker [ 26 ] interleaves thinking with web search and deep web exploration. HiRA [ 20 ] introduces a hi- erarchical agent architecture where a meta planner decomposes tasks, a coordinator routes subtasks, and specialized executors solve them with dual-channel memory. OpenAI Deep Research [ 34 ] is an agentic system based on reasoning models. 4.3 Implementation Details We use QwQ-32B [ 51 ] as DeepAgentâ€™s backbone model, with Qwen2.5- 32B-Instruct [ 40 ] as the auxiliary model in our main results. Text generation employs a maximum of 81,920 tokens with temperature 0.7, top_p 0.8, top_k 20, and repetition penalty 1.05. Web search

DeepAgent: A General Reasoning Agent with Scalable Toolsets Conferenceâ€™17, July 2017, Washington, DC, USA 0 20 40 60 80 100 Training Step 0.3 0.4 0.5 0.6 0.7 Score ToolPO (Ours) GRPO 0 20 40 60 80 100 Training Step 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 Score ToolPO (Ours) GRPO (a) Reward Scores (b) Validation Scores Figure 4: Visualization of training dynamics, including (a) reward scores and (b) validation scores across training steps. Table 3: Ablation studies on the components of DeepAgent, where the best results are in bold. Method Tool-Usage Application Avg. ToolB. ToolH. WebS. GAIA DeepAgent-32B-RL 64.0 40.6 34.4 53.3 48.1 w/o Training (Base) 60.0 38.4 32.0 46.7 44.3 w/o Memory Folding 63.0 36.6 32.4 44.7 44.2 w/o Tool Simulation 62.0 35.2 33.6 48.5 44.8 w/o Tool Adv. Attribution 62.0 39.6 33.2 49.5 46.1 and page browsing are implemented using Google Serper API and Jina Reader API, respectively. The VQA tool is based on Qwen2.5- VL-32B-Instruct [ 1 ]. Tool retrieval is performed using bge-large- en-v1.5 [ 61 ]. Training consists of 100 steps of ToolPO with batch size 64, ğœ† 1 = ğœ† 2 = 1 , rollout size ğ¾ = 8 , and maximum sequence length 32,768. Additional details are provided in Appendix C. All experiments are conducted on 64 NVIDIA H20-141GB GPUs. 5 Experimental Results 5.1 Main Results on General Tool Usage Tasks Table 1 presents the results on general tool usage, leading to several key observations. (1) DeepAgentâ€™s End-to-End Reasoning Sur- passes Workflow-Based Methods. DeepAgentâ€™s holistic agentic process consistently outperforms rigid, predefined workflows. For instance, on labeled-tool tasks, DeepAgent-32B-RL achieves suc- cess rates of 89.0% on TMDB and 75.4% on Spotify, substantially exceeding the strongest 32B baseline scores of 55.0% and 52.6%, re- spectively. This underscores the benefit of a holistic agentic process over rigid, predefined action cycles. (2) DeepAgent Maintains Robustness in Open-Set Scenarios. This advantage is more pro- nounced in open-set scenarios where dynamic tool discovery is critical. On ToolBench and ToolHop, DeepAgent-32B-RL achieves success rates of 64.0% and 40.6%, respectively, far exceeding the top baseline scores of 54.0% and 29.0%. This demonstrates that DeepA- gentâ€™s strategy of dynamically discovering tools as needed within the reasoning process is far more robust and scalable in realistic open-set scenarios. (3) ToolPO Training Further Improves Tool- Usage Capabilities. The proposed ToolPO RL strategy provides significant further gains. The trained DeepAgent-32B-RL model consistently improves upon its base version, boosting success rates Table 4: Effectiveness analysis of autonomous tool retrieval strategy in open-set scenarios compared to pre-retrieved tool methods. Numbers in parentheses indicate toolset sizes. Method ToolB. ToolH. TMDB Spotify Avg. (16k) (3.9k) (54) (40) ReAct Workflow Input Retrieved Tool 35.0 25.4 14.0 15.0 22.4 Auto. Tool Retrieval 34.0 37.1 18.0 27.8 28.0 Plan-and-Solve Workflow Input Retrieved Tool 37.0 24.8 19.0 16.0 24.2 Auto. Tool Retrieval 45.0 25.7 24.0 19.3 28.5 End-to-end Agentic Reasoning (DeepAgent) Input Retrieved Tool 53.0 37.0 34.0 43.9 42.0 Auto. Tool Retrieval 64.0 40.6 55.0 50.9 52.6 on ToolBench by up to 6.0% and on Spotify (labeled) by 5.2%. This validates the effectiveness of the ToolPO strategy, which uses an LLM-based tool simulator and fine-grained advantage attribution. 5.2 Main Results on Downstream Applications Table 2 shows the results on downstream applications, which re- quire agents to handle long-horizon interactions in complex envi- ronments. (1) The autonomous reasoning paradigm generally outperforms workflow-based methods. On complex application tasks, methods that integrate tool usage into continuous reasoning consistently outperform rigid, predefined workflows. On GAIA, both DeepAgent-32B-Base (46.7) and HiRA (42.5) significantly ex- ceed the best workflow-based method CodeAct (34.5). Similarly, on WebShop, DeepAgent-32B-Base (32.0) substantially surpasses CodeAct (18.0). This demonstrates that long-horizon interaction tasks require deep agentic reasoning capabilities to achieve su- perior task accomplishments. (2) DeepAgent demonstrates su- perior performance across various application tasks. Deep- Agent achieves state-of-the-art performance among 32B models. On GAIA, DeepAgent-32B-RL scores 53.3 vs. HiRAâ€™s 42.5, and on ALFWorld reaches 91.8% vs. HiRAâ€™s 84.3%. This stems from Deep- Agentâ€™s seamless integration of actions into coherent reasoning, enabling end-to-end execution with autonomous memory folding, which is advantages unavailable to workflow-constrained methods. (3) ToolPO training further improves performance on down- stream applications. ToolPO training yields consistent gains over the base model. DeepAgent-32B-RL improves GAIA scores from 46.7 to 53.3 (+6.6) and ALFWorld success rates from 88.1% to 91.8% (+3.7), demonstrating that ToolPO effectively enhances reasoning and tool usage capabilities for complex task completion. 5.3 Analysis of Training Dynamics Figure 4 shows the training dynamics of DeepAgent, including the reward scores and validation scores across training steps. As shown in the figure, (1) DeepAgent trained with ToolPO achieves higher upper bounds on both reward and validation scores compared to the commonly used GRPO. (2) Moreover, the training reward exhibits less fluctuation than GRPO, demon- strating better training stability. This indicates that using tool

Conferenceâ€™17, July 2017, Washington, DC, USA Xiaoxi Li et al. 0 10 20 30 40 50 Maximum Action Limit 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Score DeepAgent ReAct 0 10 20 30 40 50 Maximum Action Limit 0.1 0.2 0.3 0.4 0.5 0.6 Score DeepAgent ReAct (a) WebShop (b) GAIA Figure 5: Scaling analysis of performance with respect to maximum action limits on WebShop and GAIA datasets. simulators instead of directly training with unstable real-world APIs, along with employing tool-call process supervision, enables more stable and effective training of tool-usage capabilities. 5.4 Ablation Studies We conduct ablation studies in Table 3 to validate the effectiveness of each component in DeepAgent. (1) Importance of ToolPO Training: Removing ToolPO training (the Base model) results in the most significant performance drop (from 48.1 to 44.3). This highlights the central role of our end-to-end RL method in enhanc- ing tool use and complex task completion. (2) Effectiveness of Memory Folding: The absence of memory folding also leads to a substantial performance decline (average score drops to 44.2), particularly on the long-horizon task GAIA (from 53.3 to 44.7). This confirms that the autonomous memory folding mechanism, allow- ing the agent to "take a breath" and replan, is crucial for robust long-term interaction. (3) Contribution of Training Strategies: Removing the tool simulator and tool-call advantage attribution both lead to performance degradation. This validates that the tool simulator enables more stable training, and fine-grained advantage attribution provides precise learning signals. 5.5 Effectiveness of Tool Retrieval Strategies To compare pre-retrieving tools versus autonomous discovery dur- ing task execution, we conduct experiments shown in Table 4. (1) The on-demand nature of dynamic tool discovery yields su- perior performance and robust scalability. Autonomous tool retrieval during reasoning consistently outperforms pre-retrieved tools across all frameworks, demonstrating the superiority of on- demand tool access in open-set scenarios. Performance gains are most pronounced on large toolsets like ToolBench (16k tools) and ToolHop (3.9k tools), indicating robust scalability for real-world tasks. (2) DeepAgent synergizes better with dynamic retrieval. Combined with autonomous tool retrieval, our framework achieves the best results by a large margin, scoring 52.6 on average versus 28.5 for the best workflow-based method. This demonstrates Deep- Agentâ€™s architecture is uniquely suited for dynamic tool discovery. 5.6 Scaling Analysis of Action Limits Figure 5 illustrates the performance of DeepAgent and ReAct on the WebShop and GAIA datasets as the maximum action limit is Table 5: Performance comparison with different reasoning model backbones, spanning MOE-based models with 30B and 235B parameters. Method Tool-Usage Application Avg. ToolB. ToolH. ALF. WebS. GAIA Qwen3-30B-A3B-Thinking ReAct 52.0 22.0 67.9 18.4 34.5 35.7 Plan-and-Solve 50.0 23.6 68.7 20.4 35.2 37.0 DeepAgent (Base) 59.0 47.5 69.4 31.4 39.4 46.9 Qwen3-235B-A22B-Thinking ReAct 61.0 40.9 79.9 21.6 36.4 45.1 Plan-and-Solve 63.0 43.0 78.4 24.4 38.4 46.0 DeepAgent (Base) 67.0 48.2 85.8 37.2 51.5 55.7 5.7 Generalization Across Different Backbones Table 5 shows the performance of DeepAgent with different back- bone large reasoning models, including Qwen3-30B-A3B-Thinking and Qwen3-235B-A22B-Thinking [ 64 ]. (1) DeepAgent consistently outperforms workflow-based methods. With both the 30B and 235B MoE-based reasoning models as backbones, DeepAgent main- tains a significant performance margin over ReAct and Plan-and- Solve, demonstrating the generalizability of its agentic reasoning approach. (2) DeepAgent scales effectively with larger models. While all methods benefit from scaling the backbone from a 30B to a 235B model, DeepAgent shows the largest absolute performance gains on complex application tasks. 6 Conclusion In this work, we introduce DeepAgent, an end-to-end reasoning agent that unifies thinking, tool discovery, and execution into a single, coherent agentic reasoning process. To enable robust long- horizon interaction, we propose an autonomous memory folding mechanism that compresses interaction history into a structured memory, allowing the agent to "take a breath" and reconsider its strategy. We also introduce ToolPO, an end-to-end RL method that leverages LLM simulated APIs for stable training and fine-grained advantage attribution for precise credit assignment to tool invoca- tions. Extensive experiments on general tool-use and downstream applications demonstrate that DeepAgent significantly outperforms various baseline agents, particularly in open-set scenarios requiring dynamic tool discovery over scalable toolsets.

DeepAgent: A General Reasoning Agent with Scalable Toolsets Conferenceâ€™17, July 2017, Washington, DC, USA References [1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-VL Technical Report. CoRR abs/2502.13923 (2025). arXiv:2502.13923 doi:10.48550/ARXIV.2502.13923 [2] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards Reason- ing Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Mod- els. CoRR abs/2503.09567 (2025). arXiv:2503.09567 doi:10.48550/ARXIV.2503.09567 [3] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024. Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs. arXiv:2412.21187 [cs.CL] https://arxiv.org/abs/ 2412.21187 [4] Yifei Chen, Guanting Dong, and Zhicheng Dou. 2025. Toward Ef- fective Tool-Integrated Reasoning via Self-Evolved Preference Learning. arXiv:2509.23285 [cs.AI] https://arxiv.org/abs/2509.23285 [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR abs/2501.12948 (2025). arXiv:2501.12948 doi:10.48550/ARXIV.2501.12948 [6] Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. 2025. Agentic Entropy-Balanced Policy Optimization. arXiv:2510.14545 [cs.LG] https://arxiv.org/abs/2510.14545 [7] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Tool-Star: Empow- ering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning. CoRR abs/2505.16410 (2025). arXiv:2505.16410 doi:10.48550/ARXIV.2505.16410 [8] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. 2025. Agentic Reinforced Policy Optimization. CoRR abs/2507.19849 (2025). arXiv:2507.19849 doi:10.48550/ ARXIV.2507.19849 [9] Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Towards General Agentic Intelligence via Environment Scaling. arXiv:2509.13311 [cs.CL] https://arxiv.org/abs/2509.13311 [10] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs. arXiv:2504.11536 [cs.CL] https://arxiv. org/abs/2504.11536 [11] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. 2025. Beyond Ten Turns: Unlocking Long-Horizon Agen- tic Search with Large-Scale Asynchronous RL. CoRR abs/2508.07976 (2025). arXiv:2508.07976 doi:10.48550/ARXIV.2508.07976 [12] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advanc- ing Reasoning. (2025). arXiv:2504.11456 [cs.CL] https://arxiv.org/abs/2504.11456 [13] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025. Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions. CoRR abs/2503.23278 (2025). arXiv:2503.23278 doi:10.48550/ARXIV.2503.23278 [14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung- Yeung Shum. 2025. Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model. CoRR abs/2503.24290 (2025). arXiv:2503.24290 doi:10.48550/ARXIV.2503.24290 [15] Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, and Jun Wang. 2025. Deep Research Agents: A Systematic Examination And Roadmap. CoRR abs/2506.18096 (2025). arXiv:2506.18096 doi:10.48550/ARXIV.2506.18096 [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al . 2024. OpenAI o1 System Card. arXiv preprint arXiv:2412.16720 (2024). [17] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, and Wenhu Chen. 2025. VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use. arXiv:2509.01055 [cs.AI] https://arxiv.org/abs/2509.01055 [18] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. CoRR abs/2503.09516 (2025). arXiv:2503.09516 doi:10.48550/ARXIV.2503.09516 [19] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. 2024. From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future. CoRR abs/2408.02479 (2024). arXiv:2408.02479 doi:10.48550/ARXIV.2408.02479 [20] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Zhao Yang, Hongjin Qian, and Zhicheng Dou. 2025. Decoupled Planning and Execution: A Hierar- chical Reasoning Framework for Deep Search. CoRR abs/2507.02652 (2025). arXiv:2507.02652 doi:10.48550/ARXIV.2507.02652 [21] Jiajie Jin, Yuyao Zhang, Yimeng Xu, Hongjin Qian, Yutao Zhu, and Zhicheng Dou. 2025. FinSight: Towards Real-World Financial Deep Research. arXiv:2510.16844 [cs.CL] https://arxiv.org/abs/2510.16844 [22] Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, and Saravan Rajmohan. 2025. ACON: Optimizing Context Compression for Long-horizon LLM Agents. arXiv:2510.00615 [cs.AI] https://arxiv.org/abs/2510.00615 [23] Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebSailor- V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning. CoRR abs/2509.13305 (2025). arXiv:2509.13305 doi:10. 48550/ARXIV.2509.13305 [24] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, Decem- ber 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 3102â€“3116. doi:10.18653/V1/2023.EMNLP-MAIN.187 [25] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large Reasoning Models. CoRR abs/2501.05366 (2025). arXiv:2501.05366 doi:10.48550/ ARXIV.2501.05366 [26] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian,