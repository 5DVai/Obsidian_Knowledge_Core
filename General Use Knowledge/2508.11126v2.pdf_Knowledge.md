# AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities
**Source:** 2508.11126v2.pdf  
**Ingestion Date:** 2025-11-28

## Executive Summary
AI agentic programming is an emerging paradigm that leverages large language models (LLMs) to autonomously perform software development tasks. Unlike traditional code generation tools, these agents can plan, execute, and refine tasks iteratively, interacting with tools such as compilers, debuggers, and version control systems. This survey provides a comprehensive overview of the field, introducing a taxonomy of agent behaviors and system architectures, and examining techniques for planning, context management, tool integration, execution monitoring, and benchmarking. The document highlights the challenges and opportunities in building reliable, transparent, and collaborative coding agents, emphasizing the potential to fundamentally change software development practices.

## Key Concepts & Principles
- **AI Agentic Programming:** A paradigm where LLM-based agents autonomously perform software development tasks.
- **Reactivity vs. Proactivity:** Reactive agents respond to prompts, while proactive agents plan and execute tasks autonomously.
- **Single-turn vs. Multi-turn Execution:** Single-turn agents respond to individual prompts, whereas multi-turn agents maintain state across interactions.
- **Tool-Augmented vs. Standalone Agents:** Tool-augmented agents integrate with external tools, while standalone agents rely solely on LLM capabilities.
- **Static vs. Adaptive Agents:** Static agents follow predefined workflows, while adaptive agents modify strategies based on feedback.

## Detailed Technical Analysis
### Architectural Patterns
- **Interactive Code Assistants:** Provide code completions and suggestions within IDEs, typically reactive and single-turn.
- **Autonomous Task-Oriented Agents:** Perform multi-step tasks with minimal human intervention, integrating external tools for validation and optimization.
- **Planning-Centric Agents:** Use structured task decomposition and execution monitoring to handle long-horizon tasks.
- **Multi-Agent and Collaborative Systems:** Involve multiple specialized agents coordinating to solve complex tasks, simulating human software teams.

### Tool Integration
- Agents interact with tools through various interfaces, including command line, REST APIs, and structured schemas. Effective integration is crucial for validating and refining generated code.

### Memory and Context Management
- Current LLMs are limited by fixed context windows. Future systems require hierarchical memory models to maintain coherence over long-running tasks.

## Enterprise Q&A Bank
1. **What is AI agentic programming?**
   - It is a paradigm where LLM-based agents autonomously perform software development tasks, planning, executing, and refining iteratively.

2. **How do agentic systems differ from traditional code generation tools?**
   - They operate in a goal-directed, multi-step manner, integrating with external tools and adapting based on feedback.

3. **What are the key challenges in AI agentic programming?**
   - Challenges include context management, tool integration, safety, and alignment with user intent.

4. **How can agents improve software development practices?**
   - By automating complex workflows, reducing maintenance costs, and augmenting developer productivity.

5. **What role do LLMs play in agentic programming?**
   - LLMs serve as the core reasoning engines, powering code generation, task planning, and natural language interaction.

## Actionable Takeaways
- **Integrate with Tools:** Ensure agents can interact with compilers, debuggers, and other development tools for validation and refinement.
- **Enhance Context Management:** Develop memory models that support long-term task coherence and structured information recall.
- **Focus on Safety and Trust:** Implement mechanisms for explaining agent decisions and ensuring alignment with user intent.
- **Leverage Multi-Agent Systems:** Explore collaborative frameworks where specialized agents coordinate to solve complex tasks.
- **Benchmark Effectively:** Use comprehensive benchmarks that reflect real-world software engineering workflows and tool interactions.

---
**Raw Content:**  
AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities HUANTING WANG, University of Leeds, UK JINGZHI GONG, University of Leeds, UK HUAWEI ZHANG, University of Leeds, UK JIE XU, University of Leeds, UK ZHENG WANG, University of Leeds, UK AI agentic programming is an emerging paradigm where large language model (LLM)-based coding agents autonomously plan, execute, and interact with tools such as compilers, debuggers, and version control systems. Unlike conventional code generation, these agents decompose goals, coordinate multi-step processes, and adapt based on feedback, reshaping software development practices. This survey provides a timely review of the field, introducing a taxonomy of agent behaviors and system architectures and examining relevant techniques for planning, context management, tool integration, execution monitoring, and benchmarking datasets. We highlight challenges of this fast-moving field and discuss opportunities for building reliable, transparent, and collaborative coding agents. CCS Concepts: • Software and its engineering → Software development techniques . Additional Key Words and Phrases: Large Language Models, LLMs, AI Agents, AI Agentic Programming 1 Introduction The software development paradigm is changing rapidly with the rise of large language mod- els (LLMs) [ 103 ]. These models enable artificial intelligence (AI) systems that not only translate natural language descriptions into code snippets [ 119 ] but also understand task requirements, interact with development tools, and iteratively refine their outputs to produce complex soft- ware [ 42 , 191 ]. Recent studies suggest that developers now use LLMs routinely to assist in daily coding tasks [ 84 , 92 , 103 ]. Unlike traditional code generation tools [ 41 ] that respond to a single prompt with a static code snippet, emerging AI coding agents are designed to operate within dynamic software environments, performing iterative, tool-augmented tasks to achieve complex goals. This shift has given rise to a new programming paradigm, AI agentic programming , where LLM-based coding agents can autonomously plan, execute, and refine software development tasks [ 88 , 176 ]. Unlike conventional code-completion tools [ 19 , 22 , 57 ], which primarily assist with local suggestions, these agents are capable of generating entire programs or modules from natural language specifications, diagnosing and fixing bugs using compiler or test feedback, writing and executing test cases, and refactoring code for readability or performance. Beyond code generation, they can also orchestrate external tools, such as compilers, debuggers, performance profilers, and version control systems, supporting an end-to-end development workflow. This emerging paradigm has the potential to fundamentally change how software is built and maintained. For example, an AI agent can take a natural language description of a feature and work through a series of steps, such as writing code, generating tests, running those tests, analyzing and fixing issues, and preparing a pull request [ 191 ]. Some state-of-the-art coding agents, like Anthropic’s Calude Opus 4 [ 4 ], have demonstrated the ability to continue working for hours while maintaining task consistency, avoiding deadlocks, and recovering from failed actions [ 176 , 191 ]. Authors’ Contact Information: Huanting Wang, H.Wang7@leeds.ac.uk, University of Leeds, Leeds, UK; Jingzhi Gong, J.Gong@leeds.ac.uk, University of Leeds, Leeds, UK; Huawei Zhang, schz@leeds.ac.uk, University of Leeds, Leeds, UK; Jie Xu, J.Xu@leeds.ac.uk, University of Leeds, Leeds, UK; Zheng Wang, z.wang5@leeds.ac.uk, University of Leeds, Leeds, UK. arXiv:2508.11126v2 [cs.SE] 15 Sep 2025

2 Wang et al. These systems can generate and test code, migrate software between frameworks, debug runtime failures, and integrate new features by decomposing complex goals into manageable subtasks [ 73 , 157 ]. This represents a clear shift from static, one-shot AI-based code generation to interactive, iterative, and tool-augmented workflows . Although progress has been fast, AI agentic programming is still in its early stages. Existing systems vary in architecture, autonomy, tool integration, and reasoning capabilities. There is no standard taxonomy, benchmark suite, or evaluation methodology. At the same time, multiple key challenges remain. These include ensuring reliability and robustness in dynamic environments [ 103 ], mitigating errors and hallucinations in generated code [ 92 ], extending support beyond dominant languages such as Python to diverse platforms and software ecosystems [ 237 ], and embedding safety, trust, and accountability into autonomous behaviours [242]. The success of AI coding agents also depends heavily on their ability to interact effectively with external tools. However, today’s programming languages, compilers, and debuggers are fundamentally human-centric [ 40 , 167 ]. They are not designed for automated, autonomous systems. These tools often abstract away internal states and decision-making processes to improve usability, ensure portability, and reduce cognitive load for human users [ 133 , 164 ]. While this abstraction benefits human developers, it may not fit AI agents, which require fine-grained, structured access to internal states, transformation sequences, and validation logic in order to reason about the effects of their actions [ 55 ]. Without such access, AI agents struggle to diagnose failures, understand the implications of their changes, or recover from errors in a principled way. For instance, when a code transformation leads to a build failure, the agent needs more than just an error message - it must trace the failure to specific intermediate steps and understand why certain code edits or actions caused the issue. Existing development environments do not provide hooks and feedback mechanisms to support this kind of iterative, tool-integrated reasoning. Similarly, agentic coding systems would benefit from toolchains that support iterative development , state tracking , and rich feedback propagation - capabilities that most conventional tools do not expose. To operate effectively, AI agents may need access to internal compiler representations, transformation traces, symbolic information, and execution metadata. These challenges show that AI agentic programming is not just a new way of using existing tools. It is a shift that exposes important gaps in how today’s systems software is designed. As the field evolves rapidly, there is an urgent need to clarify its conceptual landscape, identify common patterns and system architectures, and assess the suitability of current development ecosystems. This is the right moment to step back, take stock of recent progress, and lay out the key questions that researchers and developers need to tackle next. Therefore, this survey aims to provide a comprehensive overview of the emerging field of AI agentic programming. Specifically, it covers: • A conceptual foundation and taxonomy of AI coding agents, • A review of core system architectures and underlying techniques, • A summary of the behavior dimensions, operating modes, evaluation strategies and benchmarking practices of AI coding agents, • A discussion of key challenges and current limitations, and • An exploration of future research directions, including opportunities to bridge perspectives across disciplines such as programming languages, software engineering, AI, and human-computer interaction. We focus primarily on LLM-driven agentic systems for software development , though many insights extend to general AI agents in other domains like information retrieval [ 144 ]. Our goal is to chart the current landscape, clarify foundational concepts, and support the design of robust, efficient, and trustworthy AI agents for programming.

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 3 Users Machine Copilot OS Context Streamed Result Prompt Workspace Summarized Workspace Loop Tools Read_file Edit_file Run_in_terminal ... Tool Call LLM Prompt Tool Call Result . . . Results Fig. 1. An example workflow of an AI coding agent. 2 Background 2.1 AI Agentic Programming AI agentic programming refers to a new programming paradigm in which LLM-based agents autonomously perform software development tasks. Unlike traditional code generation tools that produce outputs in a single step based on a static prompt [ 237 ], agentic systems operate in a goal-directed, multi-step manner. They reason about tasks, make decisions, use external tools (such as compilers, debuggers, and test runners), and iteratively refine their outputs based on feedback [ 150 , 176 , 205 ]. These agents can plan sequences of actions, adapt their strategies over time, and coordinate complex development workflows with limited or no human intervention. At its core, agentic programming combines the capabilities of natural language processing, external tool integration, and task planning. Figure 1 illustrates the architecture of a GitHub Copilot-style agentic programming system [ 89 ]. At its core, the agent embeds an LLM within an execution loop, enabling interaction with the development environment. The LLM receives natural language prompts from the user and gathers additional context from the operating system and the workspace (e.g., file summaries or environment state). This information is passed into the reasoning loop, where the LLM decomposes the task into subgoals, generates code or decisions, and determines whether to invoke external tools like reading/editing files or executing terminal commands. Tool outputs are returned to the loop and used as feedback for further refinement. This iterative process continues until the agent completes the task or reaches a stopping condition. Final results are streamed back to the user. A typical AI agentic programming system is characterized by several key properties. First, it emphasizes autonomy , where LLM-based agents can make decisions and take actions without continuous human supervision. Second, it is inherently interactive , as agents engage with external tools and environments during execution. Third, it supports iterative refinement , allowing agents to improve outputs based on intermediate feedback. Importantly, it is goal-oriented , with agents pursuing high-level objectives (e.g., sub-tasks generated from the user inputs) rather than simply responding to one-shot prompts. Together, these features mark a departure from earlier forms of automation and code generation based on rules [ 238 ], classical machine learning models [ 41 ] or one-shot LLM calling [ 237 ]. AI agentic programming represents a change toward intelligent systems that actively participate in the software development process. This enables new capabilities in intelligent code assistance [ 89 ], autonomous debugging and testing [ 73 , 81 ], automated code maintenance [ 85 ], and potentially even self-improving software systems [105, 246]. 2.1.1 Working example. As an example of AI agentic programming, consider a developer who tasks an AI coding agent with the following request: “Implement a REST API endpoint that returns the top 10 most frequently accessed URLs from a web server log file. Include unit tests and documentation.”

4 Wang et al. Tools Prompt Inputs Feedback Python functions Call LLM planning . . . Pytest Run & Validate . . . Sphinx documentation . . . Users Copilot LLM Documents Fig. 2. Agentic workflow for implementing a REST task. Program synthesis Code completion tools Predict next tokens Local context Formal specifications symbolic search, logic programming 1960s 1980s System evolution Process Threads Active execution unit Passive encapsulation Objects Agentic programming systems Planning + tool calling + multi - turn feedback Autonomous agents 2000s 2010s GPT, Codex, Claude, LLaMA, StarCoder Few - shot / zero - shot LLM - based assistants Algorithm evolution 1990s Active encapsulation Active objects Fig. 3. The evolution of coding agents from program synthesis, to code completion tools, to AI coding agents. This task requires integrating multiple software components, including file parsing, frequency analysis, web API implementation, testing, and documentation. A high-level view of an agentic loop for solving this task is depicted in Figure 2. Here, an LLM begins by analyzing the natural language task and planning a sequence of actions. It first produces a Python function to parse the log file and count URL frequencies using a dictionary or a data analysis library like collections.Counter . Next, it implements a REST API endpoint using a web framework such as Flask , exposing a route like /top-urls that returns the computed result in JSON format. The LLM then writes unit tests and calls a Python interpreter to execute the generated Python script in the terminal and collects output to validate both the parsing logic and the API usage. It runs these tests using a tool like pytest , identifies failing cases, and refines the implementation. If a test fails due to a corner case (e.g., missing fields or malformed input), the LLM goes back to change the Python code, e.g., by adding input validation. It repeats this process - running tools, interpreting results, and modifying the code - until all tests pass. Finally, the LLM can generate documentation strings for each function and call an external tool like pdoc or Sphinx to produce human-readable API documentation. The process concludes when the agent validates that the tests pass, the API behaves as expected, and the documentation is complete. This example illustrates the core features of agentic programming: autonomous planning, tool integration, iterative refinement, and goal-directed behavior. Unlike one-shot code generation, the agent interacts continuously with tools, learns from feedback, and adapts its actions to deliver a complete and functional software component. 2.2 Historical Context and Motivations 2.2.1 A systems perspective. As illustrated in Figure 3, the idea of automating software development has long been a goal in artificial intelligence and software engineering research [ 207 ]. In some respects, the rise of AI agents echoes the historical evolution of operating systems. Early operating systems introduced processes as the basic active execution units, capable of running independently.

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 5 Input Candidate examples ... Machine learning model Large language model Unit tests Multi - agent Output Tools integration (e.g. doc and check) ... Program Synthesis Code Completion Tools Pretrained LLMs Agentic Programming Searching Next token prediction Fig. 4. Evolution of code generation approaches. These evolved into threads, lightweight abstractions that enabled efficient concurrency and finer- grained scheduling [ 233 ]. Later, with object-oriented design [ 197 ], objects became the dominant unit of modularity - encapsulating state and behaviour, but remaining fundamentally passive. Active objects [ 173 , 174 ] extended the object model by incorporating their own thread of control and asynchronous message passing, allowing them to act autonomously rather than merely responding to external calls—an early precursor to modern agents. LLM-based AI agents can be seen as the next step in this trajectory. Like processes and threads, they are active entities, but unlike objects, they do not wait passively for instructions at every step. Instead, they can plan, act, and coordinate tasks proactively across diverse contexts. Their behavior is not restricted to fixed, handwritten policies but is driven by LLMs, enabling them to adapt dynamically, invoke external tools, and collaborate with both humans and other agents [ 145 ]. In this sense, agentic programming represents a new stage in the long-standing pursuit of making software development more autonomous, where code generation, debugging, and optimization are no longer fully hand-crafted but emerge through iterative interactions between humans, tools, and intelligent agents. 2.2.2 Algorithm evolution of agents. Figure 4 shows the evolution of algorithms. Early efforts in program synthesis aimed to generate correct-by-construction programs from formal specifica- tions [ 94 ], while code completion tools sought to improve developer productivity by predicting likely code snippets based on contexts [ 44 , 89 , 215 ]. These approaches, while impactful, typically relied on classical machine-learning models [ 41 ], handcrafted ruless [ 238 ], or statistical techniques with limited generalizations [58]. The advent of large-scale pre-trained LLMs such as Codexs [ 179 ], StarCoder [ 160 ], LLaMas [ 168 ], GPTs [ 180 ], Claudes [ 49 ], Gemini [ 82 ], Qwen [ 111 ], and Deepseek [ 95 ], marked a major turn- ing point. These models demonstrated strong zero-shot and few-shot capabilities in generating code [ 136 , 188 ], translating between programming languages [ 64 , 96 ], and answering complex pro- gramming questions with little or no task-specific fine-tuning [ 67 , 146 ]. Their ability to understand and generate natural language and code made them a good fit for software development tasks beyond basic code completion [ 113 ], including documentation generation [ 80 ], test synthesis [ 123 ], and bug detection and fixing [ 73 , 81 , 125 ]. As these models became more capable, a new opportunity emerged: using LLMs not just as passive code generators, but as autonomous agents that could reason about goals, invoke tools, and refine their outputs over multiple steps. This shift leads to the paradigm of AI agentic programming, where models operate as task-driven entities capable of planning, interacting with compilers and debuggers, and self-correcting based on feedback.

6 Wang et al. Several trends motivated this change. First, real-world software development often requires iterative problem solving, tool use, and adaptation, which single-step code generation cannot handle effectively [ 145 , 150 ]. Second, the rise of prompt engineering and structured prompting techniques (e.g., ReAct, chain-of-thought, scratchpads) enabled LLMs to reason more effectively over multiple steps [ 95 , 243 ]. Third, the increasing availability of APIs, command-line tools, and language server protocols made it possible to integrate LLMs into full-stack development environments [ 50 , 52 , 66 ]. These developments prompted a rethinking of how LLMs could be deployed-not just as smart coding assistants [ 89 , 215 ], but as semi-autonomous agents capable of carrying out software engineering tasks with minimal human supervision. 2.3 Agency in AI Systems Agency is a foundational concept in the design of intelligent systems. At its core, an agent is an entity capable of perceiving its environment, reasoning about goals, and taking actions to influence outcomes. In the context of AI coding agents, agency refers to a system’s capacity to act autonomously, i.e., selecting actions based on internal objectives, external feedback, and learned knowledge. Classical AI research has explored agency extensively in domains such as planning, robotics, and multi-agent systems [ 145 , 235 ]. Traditional agent models are typically characterized by four key attributes: reactivity , the ability to respond to changes in the environment; proactivity , the pursuit of long-term goals; social ability , the capacity to communicate and coordinate with other agents or humans; and autonomy , the ability to operate without direct human intervention. In the context of AI agentic programming, these notions of agency are realized in new ways. An LLM-based system can interpret open-ended tasks expressed in natural language, plan sequences of development steps such as writing, testing, and debugging, and invoke or coordinate external tools like compilers, test runners, and linters. It can also adapt its actions in response to environmental feedback, such as compiler errors or test failures, while maintaining coherent state and reasoning across multiple iterations. Unlike symbolic AI agents that rely on explicitly defined world models and search-based planning, LLM-based coding agents operate in a probabilistic, language-driven manner. Despite this difference, they increasingly exhibit behaviors aligned with classical definitions of agency, especially when augmented with memory, tool-use modules, and planning routines. 2.4 Key Enablers of Agentic Programming Figure 1 shows a representative system architecture of AI agentic programming. The emergence of AI agentic programming has been made possible by a combination of advances in language modeling, interaction frameworks, and software toolchains. Together, these enablers allow LLMs to move beyond static code generation toward goal-driven, interactive behavior. Below, we summarize the core technical factors that underpin this transition. 2.4.1 Large language models. LLMs trained on massive corpora of code and natural language form the foundation of modern agentic programming systems. These models, as represented by GPT-5 [ 181 ], Claude [ 49 ], DeepSeek [ 95 ], and Gemini [ 46 ], serve as the core reasoning engines, powering code generation, task planning, debugging, documentation, and natural language interaction. Their ability to understand and execute complex instructions makes them central to the design of agentic workflows. Modern LLMs can generate syntactically correct and semantically meaningful code, answer development-related queries, and engage in multi-turn conversations with minimal task- specific fine-tuning. Many of these models leverage few-shot, zero-shot, and in-context learning capabilities, allowing them to generalize across programming languages, frameworks, and task domains. This flexibility enables developers to use the same underlying model for a wide range of

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 7 Table 1. Representative LLMs for coding tasks. Model Size (B) Context Win. Tool use Provider (access) Open Weight MoE Used in coding IDEs GPT-5 N/A 1 M ✓ OpenAI (API only) ✗ N/A VS Code, Cursor, other IDEs GPT-4 variants (o3, o4, etc.) N/A 128k ✓ OpenAI (API only) ✗ ✗ VS Code, JetBrains, Cursor Claude 4 Opus ∼ 300 200k ✓ Anthropic (API only) ✗ ✗ Cursor, Replit (chat) Gemini 2.5 Pro ∼ 200 1M ✓ Google (API only) ✗ ✓ Replit, Google Colab Grok 4 ∼ 1.7T ∼ 128k ✓ xAI ✗ N/A Not publicly integrated DeepSeek R1-0528 671 (act. 37) 160k ✓ DeepSeek (API + weights) ✓ ✓ Emacs, VS Code (via ex- tension) Kimi K2 1000 (act. 32) 128k Limited Moonshot AI (API) ✓ N/A Custom plugin support Qwen3-235B- A22B 235 (act. 22) 128k Limited Alibaba ✓ ✓ Alibaba Cloud IDE Qwen3-Coder- 480B-A35B- Instruct 480 (act. 35) 256k Alibaba ✓ ✓ Alibaba Cloud IDE Solar-Pro 72 128k ✓ Upstage (weights on HF) ✓ ✗ VS Code (via third- party) Openhands-LM- 32B-v0.1 32 128k ✓ OpenHands ✓ ✗ VS Code (via extension) Devstral-Medium N/A 128k ✓ Mistral (API only) ✗ N/A VS Code (via API) Devstral-Small 24 128k ✓ Mistral (API only) ✓ ✗ VS Code (via API) software engineering tasks, from scaffolding and unit test generation to bug repair and performance tuning. In addition to general-purpose models, some LLMs like Grok [ 236 ] and Calude Opus [ 51 ] are increasingly optimized for coding tasks through specialized instruction tuning, extended context length, tool use capabilities, and integration with retrieval-based systems. These enhancements make them suitable for multi-turn reasoning, code synthesis grounded in external context, and tool-augmented workflows. Table 1 provides a comparative overview of some of the state-of-the-art LLMs used in code-related tasks. The table compares their key attributes. As the capabilities of LLMs continue to evolve, selecting and fine-tuning the appropriate foundation model for coding tasks becomes a critical design choice in building reliable, efficient, and adaptive agentic systems. 2.4.2 Prompt engineering and reasoning strategies. Effective agentic behavior often requires struc- tured prompting techniques to guide LLMs through multi-step reasoning and tool use. Rather than relying on a single input-output exchange, these methods provide scaffolding that helps models break tasks into manageable steps and maintain coherence across longer interactions. For instance, chain of thought prompting [ 231 ] encourages explicit reasoning traces, making intermediate steps visible and improving problem-solving accuracy. ReAct (reasoning and acting) interleaves reasoning with concrete actions [ 243 ], such as tool calls or environment interactions, enabling agents to both deliberate and act in context. Scratchpad prompting [ 38 ] provides a working memory space where partial results, hypotheses, or plans can be written down and refined, supporting iterative refinement. Modular prompting [ 214 ], on the other hand, separates tasks into distinct functional roles—such as planner, executor, and verifier—so that the model can coordinate across specialized subtasks. Together, these techniques allow agents to decompose complex problems, retain interme- diate states, and revise their behavior in light of new evidence. They also increase transparency by exposing the reasoning process and provide greater controllability by constraining how models structure their outputs. In practice, structured prompting forms the backbone of many agentic systems, enabling LLMs to move beyond ad hoc responses and toward more reliable, interpretable, and goal-directed behavior.

8 Wang et al. Table 2. Examples of tools supported by GitHub Copilot agent. Tool Type Examples Compiler gcc [16], clang [7], javac [20], tsc [34] Debugger gdb [14], lldb [23], pdb [28] Test Framework pytest [32], unittest [35], Jest [21], Mocha [24] Linter eslint [12], flake8 [13], black [3], prettier [30] Version Control git [15] Build System make [17], cmake [8], npm [26], maven [2] Package Manager pip [29] yarn [37], cargo [5] Language Server pyright [31], tsserver [33] import OpenAI from " openai "; const client = new OpenAI () ; const tools = [ { " type ": " function " , " function ": { " name ": " compile_code " , " description ": " ... " , " parameters ": { " type ": " object " , " properties ": { " language ": { " type ": " string " , " enum ": ["c" , " cpp "], " description ": " Programming language to compile ." }, " source ": { " type ": " string " , " description ": " Source code to compile ." }, " flags ": { " type ": " array " , " items ": { " type ": " string " }, " description ": " Compiler flags ." } }, " required ": [" language " , " source "], " additionalProperties ": false } } }, ]; Listing 1. An example of the OpenAI tool schema Table 3. Example interfaces between LLMs and tools. Interface Type Description Example Natural Language LLM sends plain text instructions; the tool parses heuristically. “Run my tests and show errors” Command Line Tools invoked with shell commands; I/O as text streams. gcc main.c -O2 Language Server Protocol JSON-RPC protocol exposing AST, symbols, diagnostics. VS Code using LSP for Python REST / gRPC APIs Tools exposed as network services with structured request/response. GitHub Actions REST API Structured Schema (JSON) Actions, parameters, outputs defined in machine-readable schema. OpenAI function calling JSON schema Intermediate Representation LLM interacts with compiler/runtime IR or AST. LLVM IR for code optimization Event Stream / Logs Tools output execution traces or state changes as structured streams. Debug logs from pytest Framework-based Adapters Middleware unifies heterogeneous tool interfaces. LangChain, Model Context Protocol 2.4.3 Tool use and API integration. Agentic systems rely heavily on external tools, such as compilers, debuggers, test frameworks, linters, and version control systems, to validate and refine generated code. These tools provide the concrete signals needed to check correctness, enforce coding standards,

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 9 and ensure that outputs remain consistent with project requirements. For example, Table 2 lists a subset of the tools currently supported by the GitHub Copilot Agent [ 89 ], covering compilation, testing, and version control. Integration with external tools can take multiple forms, including command line interfaces, language server protocols (LSP), and RESTful APIs. Increasingly, LLMs interact with tools through structured Python or JavaScript interfaces that specify the available actions, input parameters, and expected outputs in a machine-readable format. For example, Listing 1 shows how a compiler can be exposed as a tool to extend an LLM’s capabilities. This structured approach reduces ambiguity, grounds commands in the correct syntax, and makes it easier for the model to call external tools safely and consistently. By interpreting the schema, an LLM can generate well-formed commands, parse structured responses, and adjust its behavior in a predictable way. Table 3 summarizes the main types of interfaces through which LLMs interact with external tools. These range from free-form natural language instructions to highly structured schemas and domain-specific protocols. 2.4.4 State and context management. LLMs operate under fixed context windows, limiting their ability to reason over long histories. Agentic systems therefore incorporate external memory mechanisms to store plans, results, tool outputs, and partial progress. This memory can take the form of vector stores, scratchpads, or structured logs, allowing the agent to recall relevant information across multiple steps and maintain coherence over long-running tasks. Table 4 compares the context management strategies of mainstream AI coding agents, revealing substantial differences in context size and memory persistence. Tools like GitHub Copilot [ 89 ] currently do not utilize persistent memory, instead using transient methods such as sliding windows or dynamic token budgeting. In contrast, agents like SWE-agent [ 242 ], Devika [ 11 ], and OpenDevin [ 27 ] employ persistent storage, often via vector databases or structured stores, to support long-term recall of plans, tool outputs, and project history. Some, such as Cursor IDE [ 70 ] and Continue.dev [ 9 ], use embedding-based search to retrieve semantically relevant content, while others summarize prior actions to stay within the available context window. These differences reflect a clear trade-off: smaller context windows typically rely on lightweight retrieval or summarization, whereas larger windows with persistent memory enable richer state tracking but add storage and retrieval overhead. 2.4.5 Feedback loops and self-improvement. Agentic programming leverages feedback to refine outputs iteratively. Agents may rerun failed tests, revise prompts based on compiler errors, or reflect on past failures to improve future behavior. For instance, compiler errors may trigger targeted code edits, test failures may prompt iterative debugging, and linter warnings may guide stylistic refinements. Some systems incorporate explicit planning, retry mechanisms, or even gradient-based updates [ 109 ] through fine-tuning or reinforcement learning. This closed-loop design supports robustness and adaptability in complex programming tasks. 2.5 Comparison to Related Paradigms AI agentic programming represents a distinct paradigm that builds upon but fundamentally differs from existing paradigms that have shaped the landscape of automated software development. 2.5.1 Program synthesis. Program synthesis has been a foundational approach to automated code generation, traditionally divided into two types: deductive synthesis uses formal specifications to generate provably correct programs, while inductive synthesis learns from input-output examples with symbolic search and logic programming techniques to infer program logic [ 94 , 116 ]. Classical synthesis systems like sketching [ 207 ] and more recent neural approaches like RobustFill [ 76 ] specialize in generating targeted code snippets that satisfy precise specifications. However, classical program synthesis focuses on single-function generation from formal specifications (due to the

10 Wang et al. Table 4. Context management mechanisms supported by mainstream AI coding agents. Agent Underlying Model Context Window (default) Persistent Memory Context Management Mechanism GitHub Copilot GPT-4 (o-series) 16k ✗ Sliding window over active buffer Codeium GPT-4 / Claude 3.5 32k ✗ Dynamic token budgeting based on file proximity and edit history Cursor IDE Claude 3.5 Sonnet / GPT-4 128k ✓ Semantic search over project history SWE-agent GPT-4 16k ✓ Vector DB retrieval for tool outputs and plan state Devika GPT-4 / Open LLM 32k ✓ Structured memory via SQLite and embeddings AutoDev GPT-4 16k ✓ Summarization of prior actions and tool logs Continue.dev GPT-4 / Claude 32k ✗ Embedding-based local recall over recent edits OpenDevin GPT-4 / Claude / Mixtral 32k ✓ RAG over command history, plans, and intermediate outputs scaling challenge of code sysnthesis [ 94 ]) and typically operates in a one-shot generation mode [ 76 ], whereas agentic programming handles multi-step workflows (e.g., planning, tool use, and iterative refinement) and engages in continuous interaction with development environments [56]. 2.5.2 Code completion tools. Code completion, as one of the most commercially successful ap- plications of AI in programming, excels at context-aware code suggestion, leveraging large-scale pre-training on code repositories to predict next tokens of partially written code [ 113 , 146 , 175 ]. Ad- vanced completion tools can suggest entire functions, classes, or small modules based on comments, function signatures, and surrounding context, with tools like GitHub Copilot [ 89 ], TabNine [ 215 ], and Amazon Q Developer [ 44 ] achieving widespread adoption. These code completion tools often operate as reactive assistants that respond to developer input, while agentic programming systems demonstrate proactive behavior and autonomous planning. Furthermore, agentic programming extends beyond code generation to encompass testing, debugging, deployment, and maintenance activities that completion tools typically do not address [56, 242]. 2.5.3 DevOps automation. DevOps automation focuses on streamlining software delivery pipelines through Infrastructure as Code (IaC), Continuous Integration/Continuous Deployment (CI/CD), and automated testing frameworks [ 112 ]. Tools like Jenkins [ 190 ], GitLab CI [ 114 ], and modern platforms like GitHub Actions [ 90 ] automate repetitive deployment tasks, testing workflows, and infrastructure management. While both paradigms emphasize automation, DevOps automation primarily handles pre-defined workflows and infrastructure management, whereas agentic program- ming focuses on adaptive problem-solving and creative solution generation. Additionally, agentic programming can potentially orchestrate and improve DevOps processes themselves, representing a higher-order form of automation [124]. 2.5.4 Automated machine learning and automated development. Automated Machine Learning (AutoML) represents a successful paradigm for democratizing AI model development through automation of model selection, hyperparameter tuning, and feature engineering [ 99 ]. Platforms like Google Cloud AutoML [ 65 ], Amazon SageMaker Autopilot [ 202 ], and open-source frameworks like Auto-sklearn automate the traditional machine learning pipeline from data preprocessing to model deployment [ 86 ]. However, AutoML focuses on statistical model optimization within well-defined machine learning workflows and operates with structured data and standardized evaluation metrics, while agentic programming tackles more general software development challenges with creative problem-solving and multi-modal reasoning. 2.5.5 Multi-Agent systems and human-AI collaboration. Traditional multi-agent systems in software development typically involve specialized agent roles working within predefined coordination protocols [ 117 ]. These systems often feature separate agents for requirements analysis, code gen- eration, testing, and documentation, coordinating through structured communication interfaces. Recent advances in LLM-based multi-agent programming have demonstrated the potential for more sophisticated collaboration, with systems like MetaGPT showing how multiple AI agents can

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 11 Automatic Search Agent Terms Academic Databases Top - Tier Venues Title/Abstract Screening Full - Text Review 7,700 papers 395 papers 141 papers 152 papers Citation Chaining Top - Tier Venues Academic Databases Inclusion/Exclusion Criteria Data Extraction Taxonomy, Challenges, Opportunities Programming Terms AI/LLM Terms OR OR OR Fig. 5. Survey methodology for academic research. 2022 (7) 5% 2023 (33) 22% 2024 (81) 53% 2025 (30) 20% Fig. 6. Distribution of academic pa- pers. simulate software development teams [ 100 ]. AI agentic programming can be viewed as an evolu- tion of multi-agent systems that incorporates human-in-the-loop collaboration and dynamic role adaptation. Unlike traditional multi-agent systems with fixed agent roles and rigid communication protocols, agentic programming systems demonstrate fluid role assignment and context-adaptive behavior. Moreover, the integration of tool use, environmental interaction, and persistent memory distinguishes modern agentic programming from earlier multi-agent approaches. Contemporary agentic systems like AutoGen [ 235 ] and CrewAI [ 79 ] enable agents to directly interact with develop- ment tools, maintain context across extended sessions, and learn from past interactions [ 145 ]. This represents a significant advancement over traditional multi-agent systems that typically operated in more constrained, simulation-based environments. 2.5.6 Comparison with robotics and reinforcement learning agents. Robotic agents typically interact with the physical world through sensors and actuators. Their perception, control, and planning mod- ules are tightly coupled with real-time feedback and often require safety guarantees. Reinforcement learning (RL) agents [ 210 ], by contrast, learn behaviors by maximizing cumulative reward through trial and error, often in simulated environments. These agents explore large state-action spaces and acquire policies over time. Agentic programming systems share features with both paradigms. Like robotic agents, coding agents must coordinate perception, such as task understanding, with actions such as code edits or tool invocations, all within an environment shaped by constraints and feedback. Like RL agents, they benefit from feedback loops, for example, test results or compiler outputs, and may employ exploration, retry strategies, or even reward-guided behavior. At the same time, coding agents differ in multiple ways. They operate in a symbolic, tool-rich environment where actions are language-based and environments, such as codebases, APIs, and test harnesses, are highly structured. Success requires reasoning not only about immediate feedback but also about abstract software goals, dependencies, and long-term coherence across multiple steps. This makes agency in programming uniquely challenging and distinct from both the physical world and simulated agents. 3 Survey Methodology This survey follows a widely-used systematic literature review (SLR) methodology [ 91 , 103 , 128 , 195 , 227 , 249 ] to provide comprehensive coverage of AI agentic programming research, as illustrated in Figure 5.

12 Wang et al. 3.1 Search Strategy We conducted automatic searches across multiple academic databases, including Google Scholar, ACM Digital Library, IEEE Xplore, SpringerLink, and arXiv.org. We also examined proceedings from top-tier venues (FSE, ICSE, ASE, ICML, NeurIPS, AAAI, etc.). As this fast-evolving field is largely dominated by industry, we also pay attention to open-source and the industry releases of relevant results and tools. Our search string combined the following term clusters using Boolean operators: • Agent terms : “AI agent” OR “agentic” OR “autonomous agent” OR “coding agent” OR “software agent” OR “intelligent agent” OR “task agent” OR “LLM agent” • Programming terms : “programming” OR “coding” OR “software development” OR “code generation” OR “software engineering” OR “developer” OR “autonomous coding” OR “soft- ware automation” • AI/LLM terms : “large language model” OR “LLM” OR “language model” OR “foundation model” OR “AI model” OR “neural code generation” 3.2 Study Selection After initial retrieval, we followed a three-stage study selection process for academic papers: (1) title and abstract screening by two independent researchers, (2) full-text review with disagreement resolution through discussion, and (3) backward and forward citation chaining to identify additional relevant studies. During the selection process, we used the following criteria: Inclusion criteria - studies were included if they met all of the following: (1) Focus on AI systems for software development with autonomous/semi-autonomous behavior (2) Demonstrate agentic behaviors: planning, tool use, iterative refinement, or adaptive decision- making (3) Present novel techniques, architectures, evaluations, or comprehensive analysis (4) Include experimental evaluation, case studies, or substantial implementation details (5) Written in English with accessible full text Exclusion criteria - studies were excluded if they: (1) Focused solely on traditional code completion without agentic behavior (2) Addressed non-programming domains (robotics, game playing, etc.) 3.3 Results Our systematic search yielded: • Initial retrieval : 7,700 papers from database searches • Title/abstract screening : 395 papers selected for full-text review • Full-text review : 141 met all criteria. • Final corpus : 152 papers included after full-text evaluation and citation chaining • Software tools and industry products : we also study a wide range of state-of-the-art AI coding agents and LLMs like GitHub Copilot Agents, GPT, Gemini, Deepseek, Qwen and Claude Opus 4. Among the 152 academic references published from 2022 to 2025 (excluding tool descriptions and websites), 5% appeared in 2022, 22% in 2023, 53% in 2024, and 20% in 2025, as shown in Figure 6, reflecting a surge in AI agent programming research following the widespread adoption of LLMs. Based on our systematic analysis, we developed a hierarchical classification of AI agentic

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 13 AI Agentic Programming Agentic Behaviour Dimensions (Section 4.1) Agent System Categories (Section 4.2) Reactivity vs. Proactivity Single-turn vs. Multi-turn Tool-Augmented vs. Standalone Static vs. Adaptive Multi-Agent and Collaborative [ 191 , 242 , 250 ] Planning-Centric [ 141 , 225 , 232 ] Autonomous Task-Oriented [ 18 , 51 , 82 , 181 , 217 ] Interactive Code Assistants [ 10 , 44 , 89 , 215 ] Proactive [ 18 , 51 , 75 , 82 , 141 , 181 , 191 , 217 , 225 , 232 , 242 , 250 ] Reactive [ 10 , 89 , 215 ] Single-turn [ 10 , 89 , 215 ] Multi-turn [ 18 , 51 , 75 , 82 , 141 , 191 , 217 , 225 , 232 , 242 , 250 ] Tool-Augmented [ 10 , 18 , 51 , 75 , 89 , 191 , 217 , 225 , 242 , 250 ] Standalone [ 82 , 141 , 215 , 232 ] Static [ 10 , 82 , 89 , 141 , 191 , 215 , 232 ] Adaptive [ 18 , 51 , 75 , 217 , 225 , 242 , 250 ] Fig. 7. Taxonomy of AI agentic programming systems. programming systems along behavioral characteristics and system architectures, as shown in Figure 7. The following section examines each category in detail. 4 Taxonomy of AI Agentic Programming AI agentic programming is an emerging paradigm that equips LLM-based systems with autonomy, enabling them to plan, execute, and refine programming tasks over multiple steps. To provide structure to the diverse and fast-evolving landscape of agentic programming systems, this section introduces a taxonomy based on key behavioral and architectural dimensions. We categorize existing systems and approaches along these axes to clarify the design space and inform future development. 4.1 Agentic Behaviour Dimensions This subsection defines the primary behavioural traits that differentiate agentic systems, forming the basis for a comparative classification. 4.1.1 Reactivity vs. proactivity. Reactive agents respond directly to user prompts or feedback without independent task planning. For example, GitHub Copilot reacts by instantly suggesting a function body based on context after users type a function header like def initial . It does not include subsequent steps like writing tests or checking generated code. Proactive agents initiate sub- tasks, form execution plans, and re-evaluate decisions, often working autonomously over extended periods. For example, a proactive LLM-based agent can decompose a task into subtasks and execute them automatically. Users providing high-level instructions, such as “ Add a user authentication module ,” will generate the login logic, update the database, integrate it with the UI, and write corresponding unit tests. 4.1.2 Single-turn vs. multi-turn execution. Single-turn agents perform actions in response to indi- vidual prompts, often without preserving context. For example, classical GitHub Copilot responds to each prompt independently, without remembering past interactions. In contrast, multi-turn agents,

14 Wang et al. such as GitHub Copilot Agent or Claude Opus 4 with tooling capabilities, maintain state across interactions, enabling iterative refinement, exploration, and goal pursuit. For instance, GitHub Copilot Agent can hold a conversation across multiple steps, remember earlier function names, and build a complete module through back-and-forth iterations between agents [89]. 4.1.3 Tool-augmented vs. standalone agents. Some agents are tightly integrated with external tools (e.g., compilers, debuggers, browsers, test frameworks), allowing them to perform code execution, validation, and correction. Others operate solely within the LLM’s reasoning capabilities, limiting their interactivity and adaptability. 4.1.4 Static vs. adaptive agents. Static agents (e.g., GitHub Copilot and Tabnine [ 215 ]) follow predefined workflows or heuristics. Adaptive agents modify their strategies using feedback from tools, user input, or environmental signals. Some employ learning mechanisms to improve over time. For example, GitHub Copilot Agent adapts its approach when test failures occur, revising its implementation or replanning subtasks. 4.2 Agent System Categories We now present a classification of current AI agentic programming systems, organised by their core functionality and architectural patterns. 4.2.1 Interactive code assistants. These are among the most widely adopted applications of LLMs in software development. These systems assist developers by providing code completions, inline documentation, editing suggestions, and simple refactorings. They are typically integrated directly into editors and IDEs, where developers interact with the underlying LLMs either through chat-like interfaces or by selecting code or comments using mouse-based interactions. GitHub Copilot [ 89 ] and Cursor [ 10 ] are two representative examples of LLM-based code assis- tants. GitHub Copilot, originally developed based on Codex trained on GitHub code repositories, offers context-aware code completions across multiple programming languages and is tightly integrated into popular IDEs like Visual Studio Code [ 36 ] and JetBrains. Cursor extends this func- tionality by embedding conversational interaction, maintaining memory of previous edits, and supporting structured command execution. Other notable systems include Amazon Q Developer [ 44 ] and Tabnine [ 215 ]. Amazon Q Developer targets developers working in cloud ecosystems, offering language-specific completions, cloud API integration, and basic vulnerability detection. In contrast to Amazon Q Developer, Tabnine takes a privacy-first approach by deploying smaller local models trained on permissively licensed code, making it attractive for organizations and developers who do not want to send their code to a remote cloud. Implementations of these systems typically exhibit reactive behavior, responding to user input without initiating their own plans or taking proactive steps. Their interactions are generally single- turn, relying on the immediate context within the code editor rather than maintaining a persistent memory of past interactions or broader development goals. Most systems in this category are tightly coupled with development tools and offer real-time assistance that fits naturally within existing programming workflows. However, they are limited in autonomy, lacking the ability to decompose complex tasks, maintain long-term state, or coordinate multi-step development activities. Despite these limitations, interactive code assistants serve as a foundational layer within the more recent agentic programming ecosystem. They are widely deployed, easy to integrate into everyday development practices, and offer immediate value to developers. Furthermore, some recent systems, such as Cursor and GitHub Copilot Agent, are beginning to incorporate features like session-level memory, persistent context, and structured task execution, gradually bridging the gap between reactive code assistants and more autonomous, multi-turn agents.

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 15 These distinctions are captured in our taxonomy (see Table 5), where interactive code assistants are compared with other categories of agentic systems across key dimensions, including autonomy, memory scope, tool integration, reasoning complexity, and interaction model. 4.2.2 Autonomous task-oriented agents. These agents perform multi-step programming tasks with minimal human intervention, often maintaining control over the entire development process from requirement interpretation to code generation and validation. They can plan, execute, and revise their own workflows in response to intermediate results or changing task requirements. Many integrate external tools such as debuggers, package managers, or search engines, enabling them to gather information, resolve errors, and optimize code without direct user guidance. Examples include GPT-5 [ 181 ], which functions as an end-to-end collaborator for long-horizon code generation and debugging; Claude Opus 4 [ 51 ], engineered for sustained agentic workflows with strong long- context and tool-integration capabilities; Google Jules [ 18 ], which can sandbox repositories, propose diffs, and execute verified changes on real projects; DevGPT [ 75 ], an open-source pipeline that converts tickets into actionable code and CI artifacts; Kimi K2 [ 217 ], a model optimized for agentic coding proficiency; and Gemini 2 [ 82 ], which emphasizes multimodal inputs and built-in planning modes. These agents are often proactive in suggesting next steps, adaptive to new inputs, and capable of maintaining continuity across extended sessions through persistent memory or context management mechanisms. 4.2.3 Planning-centric agents. Planning-centric agents approach problem-solving as a two-phase process: first, structured task decomposition, where high-level goals are broken down into smaller, more manageable steps; and second, execution monitoring, in which results are evaluated and the plan is adjusted accordingly. This approach improves the handling of long-horizon tasks. For example, CAMEL [ 141 ] employs two agents in a role-playing setup, a “user” agent and an “assistant” agent, which collaboratively refine goals and strategies, producing plans that downstream code generators can execute. Voyager [ 225 ] demonstrates this paradigm by continuously exploring an open-ended environment, generating executable skills, and refining them into reusable plans for long-term adaptability. Similarly, CodePlan [ 232 ] introduces code-form planning, where structured pseudocode serves as an explicit intermediate representation to decompose complex problems into executable steps. These agents are typically multi-turn, memory-enabled, and trading speed for robustness. 4.2.4 Multi-agent and collaborative systems. Multi-agent and collaborative systems extend agent- based programming by introducing multiple specialized agents that coordinate to solve complex software engineering tasks. This approach draws inspiration from human software teams, where each member has a distinct role, such as requirements analysis, coding, testing, or documentation, and communication protocols are established to ensure progress toward shared objectives. For example, SWE-Agent [ 242 ] employs multiple role-specific LLM agents: an “Architect” agent for high-level design, a “Coder” agent for implementation, and a “Reviewer” agent for quality assurance, which are connected through structured dialogue and shared memory. ChatDev [ 191 ] follows this paradigm by simulating an end-to-end software company, where agents take on roles such as CEO, CTO, and programmers to collaboratively design, implement, and test applications. Furthermore, AutoCodeRover [ 250 ] extends collaboration into real-world repositories, orchestrating specialized agents that autonomously navigate, edit, and validate source code across complex multi-file projects. 4.3 Summary and Comparative Table We conclude this section with a comparative summary (Table 5) of representative systems across the behavioural dimensions and categories introduced above. This taxonomy provides a lens to

16 Wang et al. Table 5. Comparison of representative AI agentic programming systems. System Category Proactivity Multi-turn Tool Use Adaptivity GitHub Copilot [89] IDE Assistant Reactive ✗ ✓ ✗ Amazon Q Developer [44] IDE Assistant Reactive ✓ ✓ ✓ Tabnine [215] IDE Assistant Reactive ✗ ✗ ✗ Cursor [10] IDE Assistant Reactive ✗ ✓ ✗ Claude Opus 4-powered agent [51] Task-oriented Proactive ✓ ✓ ✓ Google Jules [18] Task-oriented Proactive ✓ ✓ ✓ DevGPT [75] Task-oriented Proactive ✓ ✓ ✓ KimI K2-powered agent [217] Task-oriented Proactive ✓ ✓ ✓ Gemini 2-powered agent [82] Task-oriented Proactive ✓ ✗ ✗ Voyager [225] Planning Agent Proactive ✓ ✓ ✓ CAMEL [141] Planning Agent Proactive ✓ ✗ ✗ CodePlan [232] Planning Agent Proactive ✓ ✗ ✗ ChatDev [191] Multi-agent System Proactive ✓ ✓ ✗ AutoCodeRover [250] Multi-agent System Proactive ✓ ✓ ✓ SWE-Agent [242] Multi-agent System Proactive ✓ ✓ ✓ Table 6. Example token pricing for LLMs used in coding tasks (USD per 1M tokens, as of Sep. 2025). Model Input($) Output($) Context Window GPT-5 (Standard) 1.25 10.00 256k GPT-5 Mini 0.25 2.00 128k GPT-4 variants (e.g., 4o) 2.50 10.00 128k Claude 4 Opus 15.00 75.00 200k Gemini 2.5 Pro 1.25 10 1M Grok 4 3.00 15.00 256k DeepSeek R1-0528 0.55 2.19 160k Kimi K2 0.15 2.50 128k Qwen3-235B-A22B 0.22 0.88 128k Qwen3-Coder-480B-A35B-Instruct 4.5 13.5 256k Solar-Pro 0.30 0.30 128k Openhands-LM-32B-v0.1 2.6 3.4 128k Devstral-Small 0.10 0.30 128k Devstral-Medium 0.40 2.00 128k understand the capabilities and limitations of existing approaches and can guide the design of future systems. 4.4 Cost and Token Consumption Model While recent LLMs show impressive capabilities in software engineering tasks, their real-world applicability is often constrained by cost considerations. This cost is typically measured in terms of tokens consumed per US dollar for both input and output, along with additional expenses incurred by extended reasoning strategies such as Chain-of-Thought (CoT) and tool-augmented workflows. 4.4.1 Pricing dimensions. Commercial providers generally price LLM usage by input and output tokens, with rates varying across model families. Some, such as OpenAI’s GPT-5, offer multiple service tiers (Standard, Mini, Nano, Pro) with different pricing, context lengths, and throughput limits. Table 6 summarizes representative pricing. 4.4.2 Impact of reasoning strategies. To make the cost-performance trade-offs more concrete, we draw on the “Agentic workflow for implementing a REST task” example described earlier in Section 2. In that workflow, the agent receives a high-level natural language specification for a REST API endpoint, interprets the requirements, generates code, and iteratively tests the implementation until it passes the provided unit tests. We consider three reasoning strategies applied to this scenario:

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 17 Short reasoning. The model produces the implementation in a single turn with minimal interme- diate reasoning. For the REST API task, this means directly generating the endpoint code and tests without explicit planning or validation steps. This approach minimizes token usage and latency but risks missing subtle requirements. Standard CoT. The model uses a fixed-depth chain of thought to plan the implementation. In the REST API case, this involves reasoning about request handling, data validation, and error responses before generating the code. This strategy consumes significantly more tokens, roughly twice as many, compared to the short reasoning strategy, but yields a higher likelihood of producing a correct implementation on the first attempt. Tool-augmented iterative reasoning. The agent integrates code compilation and test execution into the workflow. After producing an initial version, it runs the tests, inspects any failures, and revises the code in subsequent turns. For the REST API example, this may involve multiple cycles of fixing logic errors, adjusting request parsing, and refining edge-case handling. While this maximizes accuracy and robustness, it also increases token consumption and wall-clock time substantially due to repeated code generation and analysis. In practice, the optimal choice depends on the cost-performance budget of the project. For time-sensitive or budget-constrained environments, a hybrid approach can offer a more effective balance. 5 Challenges AI agentic programming introduces a promising and complex shift in how software is developed, relying on the autonomous capabilities of LLMs. Despite recent progress, several technical and conceptual challenges remain that hinder the deployment of robust, scalable, and trustworthy agentic systems [39, 199]. 5.1 Evaluation and Benchmarking A variety of benchmarks and open-source toolkits [ 153 , 193 ] have been proposed to evaluate the capabilities of LLM-based agents across different tasks. Despite this progress, existing benchmarks commonly used for assessing coding agents, such as HumanEval [ 254 ] and SWE-Bench [ 121 ], may still be inadequate for capturing the full complexity of real-world software engineering workflows. Most of these benchmarks primarily focus on small, self-contained problems, often restricted to a small number of programming languages, such as Python [ 62 ], and generally lack support for interactive, multi-turn, or tool-integrated tasks [ 153 ]. In contrast, practical agentic systems are expected to operate over large, modular codebases, interface with third-party libraries, manage build workflow pipelines, and respond dynamically to user feedback or runtime tool outputs. As LLM-based systems increasingly incorporate reinforcement learning [ 183 , 226 ] and more advanced planning mechanisms, future benchmarks should reflect this integration. Table 7 shows the charac- teristics of tasks in SWE-Bench. Although SWE-Bench introduces project-level repositories and leverages unit tests and continuous integration (CI) for evaluation, its scope is restricted to Python. Most tasks are function- or module-level, with no support for multi-turn feedback, third-party library usage, or build pipeline management. Even for project-level tasks, interaction is limited to binary pass/fail outcomes, providing only minimal support for realistic multi-step or tool-integrated software engineering workflows. Moreover, there is a noticeable absence of evaluation frameworks designed for emerging complex use cases, such as those involving interactions with compilers and debuggers [ 229 ], where agents must reason about low-level program behavior, perform iterative transformations, or track state

18 Wang et al. Table 7. SWE-Bench characteristics. Task type Proportion Languages Interaction Multi-turn feedback Library integration Build pipeline Function-level 65% Python Unit tests ✗ ✗ ✗ Module-level 25% Python Tests and CI ✗ ✗ ✗ Project-level <10% Python Tests and CI Limited (pass/fail) ✗ ✗ across toolchains. The lack of such domain-specific benchmarks presents a significant gap in evaluating agent performance under realistic conditions. 5.2 Communication Protocols for Multi-Agent Systems Early protocols enabled one-to-one agent–to–tool interactions but did not support direct agent-to- agent communication [ 25 ]. Current practice often relies on heterogeneous web service protocols and adapters, which provide interoperability but introduce high latency, bandwidth overhead, and limited scalability [ 172 ]. Recent work [ 1 , 224 , 239 ] explores group session models, where each execution unit functions as a session or group agent coordinating multiple services, and standards efforts propose unified message formats and session semantics. However, the lack of a common protocol, inefficiencies of adapters, and challenges in managing dynamic multi-agent sessions remain open problems for scalable and dependable agentic systems. 5.3 Domain-specific Models for Agents Generic coding agents often struggle in domain-specific environments such as embedded sys- tems, high-performance computing, optimization, or formal software verification [ 137 , 159 ]. These domains typically impose stricter operational constraints and require deep integration with spe- cialized APIs, toolchains, and domain knowledge resources that are often underrepresented in general-purpose training corpora. To address these limitations, recent research has proposed domain-adapted models and task-specific learning strategies to accelerate agent performance in specialized settings [ 208 ]. For instance, some approaches have begun incorporating compiler knowledge or security-specific patterns into LLM training pipelines [ 69 , 148 ], enabling agents to reason more effectively about low-level program behavior or vulnerability patterns. In the future, developing robust and adaptable domain foundation models will be a promising direction for enabling agents to operate reliably in complex software environments, such as LLMs pretrained or fine-tuned on domain-specific data, tools, and semantics. 5.4 Safety and Privacy As agentic systems gain increasing autonomy, so does the potential for unsafe behavior. Unlike tra- ditional tools [ 6 ], agentic systems can invoke external tools, perform structural code modifications, and even commit changes without direct human oversight [ 221 , 251 ]. These capabilities introduce significant risks, including the possibility of introducing subtle bugs, propagating unsafe patterns, or violating security constraints. A critical future direction involves ensuring that agentic systems can protect users and data. For example, when agents visit private repositories or are deployed in cloud-integrated environments, future models may need built-in controls to restrict access to sensitive project data. Further, malicious prompts, poisoned APIs, or compromised toolchains can mislead agents into executing unsafe behaviors [ 72 , 247 ]. Future research should prioritize the design of secure protocols for agent collaboration, including authentication between agents, validation of tool outputs, and detection of anomalous actions. Also, agents must be capable of explaining their reasoning, flagging uncertainties, and allowing developers to understand and

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 19 revise with minimal effort. Building safety and privacy into the foundation of agentic architectures is essential. 5.5 Toolchain Integration and Programming Language Design One fundamental challenge is the incompatibility between existing software tools and the needs of autonomous coding agents. Most programming languages, compilers, debuggers, and development environments were designed for human developers [ 198 ]. They emphasize usability and readability over structured, machine-consumable feedback. As a result, agents often struggle to diagnose failures, trace the consequences of code transformations, or interpret build errors [ 127 ]. For example, compilers typically report transformation failures or semantic conflicts in the form of coarse error messages, providing little insight into why an optimization was blocked or how a type error emerged [ 143 ]. Languages similarly prioritize human readability over machine-negotiated meaning, while compilers conceal internal reasoning to avoid overwhelming human users. These design choices, while historically effective, now limit the ability of AI coding agents to construct safe, efficient, and adaptive software. To enable agentic workflows, toolchains should evolve to expose richer intermediate representa- tions, transformation traces, and structured feedback interfaces. Equally important, programming languages should incorporate annotations and agent-aware interfaces that make developer intent explicit, allowing automated reasoning to be guided by semantic contracts rather than inferred heuristics. Together, these advances would transform compilers and languages from opaque tools into collaborative infrastructures capable of supporting autonomous agents at scale. 5.6 Scalable Memory Agentic programming systems must maintain coherence and reasoning over long-running tasks involving multiple iterations, tools, and contextual dependencies. However, current LLMs are limited by fixed context windows and lack persistent, structured memory mechanisms [ 228 ]. Realistic software tasks may require agents to store and reason over evolving states, feedback logs, intermediate plans, and prior actions [ 192 ]. Without hierarchical and queryable memory systems, agents risk repeating errors, forgetting past successes, or producing inconsistent results. Emerging solutions such as retrieval-augmented generation and memory summarization offer partial relief, but they remain inadequate for complex, multi-session workflows [ 162 ]. Future research can explore memory architectures that differentiate short-term interactions, mid-term subgoals, and long-term domain knowledge. 6 Opportunities and Future Directions AI agentic programming represents a fast-evolving research frontier that intersects artificial intelli- gence, programming languages, and software engineering. While recent advances have demon- strated promising capabilities, significant challenges remain in realizing robust, efficient, and trustworthy agentic systems. In this section, we outline several key opportunities and open re- search directions that can shape the future of this field. An overview is provided in Figure 8. 6.1 Integrating Coding Agents with Tools Existing AI coding agents typically orchestrate LLMs with loosely integrated toolchains and basic memory mechanisms. These ad hoc designs often lack robustness, scalability, and generalization across programming tasks. Advancing agent architectures will require moving beyond simple prompt-response patterns toward more modular, structured systems that support reasoning, tool interaction, planning, and verification.

20 Wang et al. Software Compatibility (Section 6.1) Structured feed- back on failures Direct IR interaction Standardized com- piler interfaces How do coding agents integrate with tools? Context Man- agement (Section 6.2) Structured storage and recall of evolving task information Conditioned retrieval from task state and tool feedback Program state trac- ing and replay How do coding agents manage memory and contextual information? Benchmarks (Section 6.3) Heavily biased training corpus Large absence of real-world tasks Lack of reflecting interactive and iterative programming workflow How to comprehensively evaluate coding agents? Human-AI Collaboration (Section 6.4) Frameworks for responsibility allocation Handle uncertainty; Ecosystem integration Coordination in multi- user environments How do coding agents collaborate with human? Domain- specific Agents (Section 6.5) Adaptive prompting, fine-tuning, and behavior modulation Reasoning-loop integration of domain tools and diagnostics Runtime domain de- tection and adaptation How to develop domain- specific coding agents? Safety, Alignment and Trust (Section 6.6) Agent behav- ior grounding Risk detection and reduction Explainable and self-reflective agents How to ensure safer, ethical, and trustwor- thy coding agents? Multi-Agent Collaboration (Section 6.7) Native protocol support for multi-agent systems Session management for dynamic coordination Semantics-aware routing, conflict resolu- tion, and consistency How to design standard- ized protocols for multi- agent collaboration? System Support (Section 6.8) System resource scheduling (CPU, GPU, memory) Reducing overhead in communication with environments Coordinating tasks and resolving conflicts in collaborative codebases How to enhance system-level support for coding agents? Fig. 8. Summary of potential future research directions of AI agentic programming. A promising direction is to rethink how programming languages, compilers, and testing frame- works, which are traditionally built for human developers, can be redesigned to support AI coding agents. For example, instead of emitting opaque diagnostics, compilers could provide structured feedback explaining why certain optimizations (e.g., vectorization or inlining) fail [ 53 , 163 , 206 ]. These could include semantic barriers like unresolved aliasing, ambiguous data/control flow, or missing annotations [ 55 ], enabling agents to revise code more precisely. Beyond diagnostics, com- pilers can help agents track state across iterations. Feedback on which edits introduced errors, failed assertions, or performance regressions would enable agents to reason over the change history and adjust strategies accordingly. Opening compiler internals also presents a valuable opportunity. Coding agents could interact directly with intermediate representations (IRs), such as LLVM IR [ 134 ] or MLIR [ 135 ], to reason about program structure, verify transformations, or perform static analysis at a semantic level. Compiler APIs and language servers (e.g., Clang’s LibTooling, the Language Server Protocol) already expose ASTs, symbol tables, and refactoring tools, but a wider adoption may require standardized, introspective interfaces across compilers. At the programming language level, agent-aware extensions or annotations could further improve interaction. Developers might use domain-specific languages, embedded contracts, or even natural language comments, e.g., “sort the elements of input array 𝑥 ”, to convey intent. This could guide synthesis, verification, or debugging. Likewise, compilers might expose symbolic summaries of control flow, memory access patterns, or performance profiles to inform multi-step agent reasoning. Tighter integration with runtime systems also offers opportunities. For instance, agents can dynamically insert instrumentation or launch profiling runs, then use the results to inform opti- mization choices. Coupling these capabilities with autotuning frameworks [ 47 , 63 ] would expand

AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities 21 the design space while preserving correctness and safety. Finally, advances in structured code representations, such as ASTs, graph-based IRs, and semantic embeddings, offer a foundation for more powerful agent reasoning. Combining LLMs with graph neural networks or neuro-symbolic systems could improve generalization and support cross-language, cross-target understanding. 6.2 Scalable Memory and Context Management A key capability of agentic programming lies in managing memory and contextual information across tasks that involve long context reasoning and multiple iterations. Unlike traditional code generation, which typically follows a single pass prompt to solution model, agentic workflows for solving real-world software engineering problems involve multiple steps, iterative refinement, and integration with external tools and development environments [10, 89, 179, 230]. Consider an agent tasked with adding a new feature to an open source project, such as imple- menting a command-line flag to enable verbose logging. The agent must first analyze the existing codebase to locate the argument parsing logic, generate the required code changes, and update the logging behavior. If the updated code fails with a runtime error due to an uninitialized flag, the agent needs to debug the issue by inspecting stack traces, revise the code accordingly, and rerun the tests. Once the implementation is verified, the agent writes a commit message, creates a pull request with a summary of the changes, and links it to the relevant issue. Throughout this process, the agent must persist and reason over a large and evolving context: the initial task description, previously generated code, compiler and runtime feedback, and version control metadata. Without the ability to store and recall this information in a structured way, the agent may repeat past mistakes, forget earlier successful changes, or submit incomplete solu- tions. Agentic programming, therefore, needs mechanisms for memory and context tracking that go beyond simple token limits, enabling coding agents to maintain continuity across extended interactions and tool usage. As the memory footprint of LLMs grows linearly with input token length [ 126 , 223 ], current LLM-based agents remain constrained by their context windows and lack persistent memory across a long sequence of iterations [ 152 , 184 , 200 ]. However, reasoning about real-world programs often requires modeling complex data structures and code context (like function calls) spanning across multiple files, which often exceeds typical context limits. Although some industry-scale LLMs claim to support million-token contexts [ 77 , 78 , 216 ], they often rely on random sampling techniques [ 102 ] and fail to leverage program structure or semantics effectively. Therefore, an interesting direction is to design attention mechanisms that are guided by code structure, such as syntax trees, control flow graphs, or data dependencies. These structures can help agents focus more accurately on the most relevant parts of a program. While approaches like retrieval-augmented generation (RAG) [ 140 ], KV cache offloading [ 138 , 211 ], and compression [ 60 , 158 ] provide partial solutions, they struggle to provide precise control over long-term dependencies, structured knowledge, and execution histories. AI coding agents can also benefit from hierarchical memory models that distinguish between short-term interaction history, mid-term planning objectives (such as subgoals and intermediate decisions), and long-term knowledge. This long-term layer may include patterns of success or failure, reusable code templates, and observed tool behaviors. Such hierarchies can be dynamically updated and selectively queried using retrieval controllers or attention-based mechanisms. Additionally, memory summarization techniques could be explored to condense lengthy interaction histories into structured, semantically meaningful representations. For example, an agent might summarize a multi-turn session as a sequence of planning decisions and outcomes, highlighting key insights and interventions.

22 Wang et al. Table 8. Benchmarks for evaluating LLMs and agentic systems on programming tasks. Abbreviations: CP = Competitive Programming, BF = Bug Fixing, FC = Function Completion, CR = CLI Reasoning, PO = Performance Optimization. Benchmark Source Language Task Difficulty Year HumanEval Hand-written Python FC Beginner 2021 MBPP Crowd-sourced Python FC Beginner 2021 CodeContests CP Python/C++/Java FC Diverse 2022 HumanEval-X Hand-written Python/C++/Java/JS/Go FC Intermediate 2023 SWE-Bench GitHub Issues Python BF Expert 2024 SWE-bench M GitHub Issues JS BF Diverse 2024 LiveCodeBench CP (live) Python FC, BF Diverse 2024 Terminal-Bench Community-curated Shell CR Diverse 2025 Spider 2.0 Enterprise DB Apps SQL FC Expert 2025 EffiBench-X Synthetic Python/C++/JS FC, BF Diverse 2025 Web-Bench Web App Projects JS/TS/HTML/CSS/Python BF, FC Expert 2025 ProjectEval Open-source Repos Python/Java/C++/JS FC, BF, CR Expert 2025 TRAIL CP Python/Java/C++/JS BF, CR Expert 2025 GSO Open-source Repos Python/C/C++/Cython/Rust PO Expert 2025 Note: "Diverse" difficulty indicates that the benchmark covers a wide range from beginner to expert-level tasks. Another important area is the development of context-aware retrieval strategies that move beyond static similarity-based methods [ 209 ]. During debugging, for instance, an agent could retrieve not only the most recent error message but also similar past failures, proposed fixes, relevant test cases, and their outcomes. Retrieval conditioned on task state and tool feedback would significantly improve the agent’s ability to reason under uncertainty. Structured mechanisms for program state tracing and replay may also enhance agent perfor- mance. By recording partial program states, tool outputs, and execution steps, agents can support backtracking, recovery from failure, and richer explanations. For example, an agent could explain how a specific code edit introduced a type error or why a particular memory access blocked loop vectorization. These capabilities are crucial for supporting causal reasoning and improving trans- parency. Likewise, persistent memory across multiple code generation and refinement sessions will be essential for enabling agents to accumulate and refine knowledge over time. This may include long-term storage of project-specific context, interaction histories, usage patterns of tools, and models of user intent. Such memory infrastructure will allow for continual learning and increasing personalization of agent behavior. In summary, effective memory and context management are foundational for scaling agentic programming systems. These capabilities are vital for advancing from reactive, prompt-driven assistants to autonomous, context-aware collaborators capable of sustained reasoning, adaptation,