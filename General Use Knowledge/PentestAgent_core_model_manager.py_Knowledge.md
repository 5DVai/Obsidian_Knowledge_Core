# AI Model Management and Token Counting in GHOSTCREW
**Source:** PentestAgent\core\model_manager.py  
**Ingestion Date:** 2025-11-28

## Executive Summary
The `model_manager.py` file is a critical component of the GHOSTCREW system, responsible for managing AI model operations and token counting. It provides a structured approach to interfacing with AI models, specifically those compatible with OpenAI's API. The file defines a `ModelManager` class that encapsulates the logic for selecting models, counting tokens, and calculating the maximum number of output tokens based on input size. This functionality is essential for optimizing AI model interactions, ensuring efficient use of resources, and maintaining performance standards.

The file also introduces a `DefaultModelProvider` class, which abstracts the process of obtaining model instances. This design promotes flexibility and scalability, allowing for easy integration of different model types or providers in the future. The utility functions for token counting and output token calculation are reusable components that can be leveraged across various parts of the application, making this file a valuable asset in the software engineering knowledge base.

## Key Concepts & Principles
- **Model Management:** Encapsulation of logic for selecting and interacting with AI models.
- **Token Counting:** Mechanism to determine the number of tokens in a given text, crucial for managing API usage and costs.
- **Output Token Calculation:** Strategy to estimate the maximum number of tokens that can be generated in response to a query, optimizing resource allocation.
- **Provider Abstraction:** Use of a provider pattern to decouple model selection from business logic, enhancing modularity and maintainability.

## Detailed Technical Analysis

### Model Management
The `ModelManager` class is designed to handle AI model operations, providing methods to count tokens and calculate output token limits. It uses a singleton pattern to ensure a single instance is used throughout the application, promoting consistency and reducing overhead.

### Token Counting
The `count_tokens` method utilizes the `tiktoken` library to encode text and count tokens. It includes a fallback mechanism to approximate token count if encoding fails, demonstrating robustness in handling edge cases.

### Output Token Calculation
The `calculate_max_output_tokens` method estimates the maximum number of tokens that can be generated based on the input size. It considers both the total token limit and a response buffer, ensuring that responses remain within acceptable bounds.

### Provider Abstraction
The `DefaultModelProvider` class abstracts the process of obtaining model instances, using configuration settings to determine the appropriate model. This approach allows for easy adaptation to different model providers or configurations.

## Enterprise Q&A Bank

1. **What is the purpose of the `ModelManager` class?**
   - The `ModelManager` class manages AI model operations, including token counting and output token calculation, ensuring efficient model interactions.

2. **How does the `count_tokens` method handle encoding failures?**
   - It falls back to an approximate token count using word splitting if the `tiktoken` encoding fails.

3. **Why is the provider pattern used in this file?**
   - The provider pattern decouples model selection from business logic, enhancing modularity and allowing for easy integration of different models or providers.

4. **What is the significance of the `calculate_max_output_tokens` method?**
   - It optimizes resource allocation by estimating the maximum number of tokens that can be generated in response to a query, considering input size and configuration limits.

5. **How does the singleton pattern benefit the `ModelManager` class?**
   - It ensures a single instance of the `ModelManager` is used throughout the application, promoting consistency and reducing resource overhead.

## Actionable Takeaways
- Implement a provider pattern to decouple model selection from business logic.
- Use token counting to manage API usage and optimize costs.
- Incorporate fallback mechanisms to handle potential failures in third-party libraries.
- Estimate output token limits to optimize resource allocation and maintain performance.
- Utilize singleton patterns for components that require consistent state across the application.

---
**Raw Content:**
```python
"""Model management and AI model setup for GHOSTCREW."""

import tiktoken
from agents import Model, ModelProvider, OpenAIChatCompletionsModel
from config.app_config import app_config
from config.constants import MAX_TOTAL_TOKENS, RESPONSE_BUFFER


class DefaultModelProvider(ModelProvider):
    """Model provider using OpenAI compatible interface."""
    
    def get_model(self, model_name: str) -> Model:
        """Get a model instance with the specified name."""
        return OpenAIChatCompletionsModel(
            model=model_name or app_config.model_name,
            openai_client=app_config.get_openai_client()
        )


class ModelManager:
    """Manages AI model operations and token counting."""
    
    def __init__(self):
        """Initialize the model manager."""
        self.model_provider = DefaultModelProvider()
        self.model_name = app_config.model_name
    
    @staticmethod
    def count_tokens(text: str, model_name: str = None) -> int:
        """
        Count tokens in the given text.
        
        Args:
            text: The text to count tokens for
            model_name: The model name to use for encoding (defaults to configured model)
            
        Returns:
            Number of tokens in the text
        """
        try:
            model = model_name or app_config.model_name
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except Exception:
            # Fall back to approximate counting if tiktoken fails
            return len(text.split())
    
    @staticmethod
    def calculate_max_output_tokens(input_text: str, query: str) -> int:
        """
        Calculate the maximum output tokens based on input size.
        
        Args:
            input_text: The base instructions or context
            query: The user query
            
        Returns:
            Maximum number of output tokens
        """
        input_token_estimate = ModelManager.count_tokens(input_text) + ModelManager.count_tokens(query)
        
        max_output_tokens = max(512, MAX_TOTAL_TOKENS - input_token_estimate)
        max_output_tokens = min(max_output_tokens, RESPONSE_BUFFER)
        
        return max_output_tokens
    
    def get_model_provider(self) -> ModelProvider:
        """Get the model provider instance."""
        return self.model_provider


# Create a singleton instance
model_manager = ModelManager()
```